[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Let’s go."
  },
  {
    "objectID": "header/refs.html",
    "href": "header/refs.html",
    "title": "Refs",
    "section": "",
    "text": "C - control. So C-a means to hit the control key and the a key.\nM - alt/meta"
  },
  {
    "objectID": "header/refs.html#terminal",
    "href": "header/refs.html#terminal",
    "title": "Refs",
    "section": "terminal",
    "text": "terminal\n\nC-p = previous command\nC-u= erase line before cursor\nC-l = refresh screen\nC-M-t = new terminal window"
  },
  {
    "objectID": "header/refs.html#vim",
    "href": "header/refs.html#vim",
    "title": "Refs",
    "section": "Vim",
    "text": "Vim\n\nsudo apt-get install vim\n\n\n.vimrc\n:inoremap jk <Esc>\nset relativenumber"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ψML",
    "section": "",
    "text": "Sep 24, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJul 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 12, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\njupyter\n\n\n\n\nBeginning the journey of training ML models on sound with rudimentary steps.\n\n\n\n\n\n\nSep 30, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\njupyter\n\n\n\n\nMy first attempt using fastpages to create a post directly from a notebook.\n\n\n\n\n\n\nAug 15, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nfastpages\n\n\n\n\nAwesome description\n\n\n\n\n\n\nAug 9, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmarkdown\n\n\n\n\nA minimal example of using markdown with fastpages.\n\n\n\n\n\n\nAug 2, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmarkdown\n\n\n\n\nA minimal example of using markdown with fastpages.\n\n\n\n\n\n\nAug 1, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nmarkdown\n\n\n\n\nA minimal example of using markdown with fastpages.\n\n\n\n\n\n\nJul 23, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmarkdown\n\n\n\n\nA minimal example of using markdown with fastpages.\n\n\n\n\n\n\nJul 21, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmarkdown\n\n\n\n\nA minimal example of using markdown with fastpages.\n\n\n\n\n\n\nJul 12, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmarkdown\n\n\n\n\nA minimal example of using markdown with fastpages.\n\n\n\n\n\n\nJul 11, 2020\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/graph/Graph_My_Code_2.html",
    "href": "posts/graph/Graph_My_Code_2.html",
    "title": "Graph My Code 2: Visualizing Function Dependencies in Python",
    "section": "",
    "text": "Writing code that visualizes code."
  },
  {
    "objectID": "posts/graph/Graph_My_Code_2.html#intro",
    "href": "posts/graph/Graph_My_Code_2.html#intro",
    "title": "Graph My Code 2: Visualizing Function Dependencies in Python",
    "section": "Intro",
    "text": "Intro\nIn the previous installment, we explored ast and gave high-level details about how to use this to generate a graph description of a Python script. Here we present a full implementation of this process: pygraphcrawler."
  },
  {
    "objectID": "posts/graph/Graph_My_Code_2.html#examples",
    "href": "posts/graph/Graph_My_Code_2.html#examples",
    "title": "Graph My Code 2: Visualizing Function Dependencies in Python",
    "section": "Examples",
    "text": "Examples\nBefore we get into how it works, let’s see it in action. To parse code in a directory and create a Mermaid description:\nfrom code_extraction import extract_code_information\nfrom viz_code import create_graph_description\n\n# m_info will house the code data\nm_info = extract_code_information(\n    directories=[\"example\", \"example2\"],  # each directory will be searched\n    other_python_filenames=[\"test.py\"]  # or specify particular files\n)\nprint(m_info.keys())  # to show us which modules we parsed\nmodule_to_inspect = \"abyss\"  # select one of the module names\n# this is a markdown description of the select module\nmermaid_graph_desc = create_graph_description(m_info[module_to_inspect])\n\nMultiple calls into submodules of import\nimport numpy as np\n\nz = np.zeroes(5)\n\ndef mul():\n    a = np.array([[1, 0],\n                  [0, 1]])\n    b = np.array([[4, 1],\n                  [2, 2]])\n    return np.matmul(a, b)\n\ndef eigs_of_product():\n    a = np.array([[1, 0],\n                  [0, 1]])\n    b = np.array([[4, 1],\n                  [2, 2]])\n    product = np.matmul(a, b)\n    eigs = np.linalg.eigs(product)\n    np.linalg.debug.depth.error_print(eigs)  # this is a fake call for testing\n    return eigs\n\n\n\n\ngraph LR;\n    mul[mul] -->|2| np.array[np.array];\n    mul[mul] -->|2| np.array[np.array];\n    mul[mul] --> np.matmul[np.matmul];\n    eigs_of_product[eigs_of_product] -->|2| np.array[np.array];\n    eigs_of_product[eigs_of_product] -->|2| np.array[np.array];\n    eigs_of_product[eigs_of_product] --> np.matmul[np.matmul];\n    eigs_of_product[eigs_of_product] --> np.linalg.eigs[np.linalg.eigs];\n    eigs_of_product[eigs_of_product] --> np.linalg.debug.dept[np.linalg.debug.depth.error_print];\n    main[main] --> np.zeroes[np.zeroes];\n\nsubgraph np\n    np.linalg.debug.dept\n    np.array\n    np.linalg.eigs\n    np.matmul\n    np.zeroes\nend\n\n\n\n\n\n\n\n\n\nexternal module calls are grouped\nrepeated calls to a function are denoted with edge labels\n\n\n\nCalls into different modules\nimport numpy as np\nimport re\n\npop_size = 10\nlow = 0\nhigh = 100\n\ndef random_draw():\n    return np.random.randint(low, high, pop_size)\n\ndef get_random_sample_mean():\n    pop = [random_draw() for _ in range(10)]\n    return np.mean(pop)\n\nIMPORTANT_PATTERN = r\"\\d\\d\\s\\w*\\s\"\ndef find(text):\n    return re.findall(IMPORTANT_PATTERN, text)\n\n\n\n\ngraph LR;\n    random_draw[random_draw] --> np.random.randint[np.random.randint];\n    get_random_sample_me[get_random_sample_mean] --> random_draw[random_draw];\n    get_random_sample_me[get_random_sample_mean] --> np.mean[np.mean];\n    py.find[find] --> re.findall[re.findall];\n\nsubgraph np\n    np.mean\n    np.random.randint\nend\nsubgraph re\n    re.findall\nend\n\n\n\n\n\n\n\n\n\nat a glance we can see connections and distinct domains of defined functions\n\nrandom_draw is a helper\nfind has nothing to do with get_random_sample_mean\n\n\n\n\nHandling Classes\nWe have functions and classes that call other classes to use their functionality. We keep track of these dependencies.\nimport numpy as np\nthings = []\n\ndef check():\n    things.append(0)\n    np.zeros(5)\n\nclass Dog:\n    def __init__(self):\n        self.friend = Catdog()\n        self.shared_total = self.friend.run()\n        test = Catdog()\n        test.run()\n\nclass Catdog:\n    def __init__(self, how):\n        self.how = how\n\n    def run(self):\n        test = np.zeros(3)\n        total =  test.sum()\n        return total\n\nc = Catdog(3)\nt = c.run()\n\ndef create_catdog_run():\n    c = Catdog(3)\n    t = c.run()\n    print(t)\n\n\n\n\ngraph LR;\n    check[check] --> np.zeros[np.zeros];\n    create_catdog_run[create_catdog_run] --> Catdog.__init__[Catdog.__init__];\n    create_catdog_run[create_catdog_run] --> Catdog.run[Catdog.run];\n    Dog.__init__[Dog.__init__] -->|2| Catdog.__init__[Catdog.__init__];\n    Dog.__init__[Dog.__init__] -->|2| Catdog.run[Catdog.run];\n    Dog.__init__[Dog.__init__] -->|2| Catdog.__init__[Catdog.__init__];\n    Dog.__init__[Dog.__init__] -->|2| Catdog.run[Catdog.run];\n    Catdog.run[Catdog.run] --> np.zeros[np.zeros];\n    main[main] --> Catdog.__init__[Catdog.__init__];\n    main[main] --> Catdog.run[Catdog.run];\nsubgraph Dog\n    Dog.__init__\nend\nsubgraph Catdog\n    Catdog.__init__\n    Catdog.run\nend\nsubgraph np\n    np.zeros\nend\n\n\n\n\n\n\n\n\n\nDog is entirely depending on Catdog. Good to know if you’re thinking about changing Catdog.\nnumpy is an important dependency, everything is connected to it\n\n\n\nDogfooding\nNow let’s apply the crawler to itself!\n\n\n\n\ngraph LR;\n    walk_script[walk_script] --> ast.parse[ast.parse];\n    walk_script[walk_script] --> worker_script.read[worker_script.read];\n    walk_script[walk_script] --> ast.walk[ast.walk];\n    get_submodule_desc[get_submodule_desc] --> get_submodule_desc[get_submodule_desc];\n    process_from_import_[process_from_import_node] --> ImportNode.__init__[ImportNode.__init__];\n    process_import_node[process_import_node] --> ImportNode.__init__[ImportNode.__init__];\n    process_call_node[process_call_node] --> print_call_node_info[print_call_node_info];\n    process_call_node[process_call_node] --> get_submodule_desc[get_submodule_desc];\n    process_call_node[process_call_node] --> CallNode.__init__[CallNode.__init__];\n    process_call_node[process_call_node] --> CallNode.__init__[CallNode.__init__];\n    process_func_def_nod[process_func_def_node] --> FuncDefNode.__init__[FuncDefNode.__init__];\n    process_class_node[process_class_node] --> ClassNode.__init__[ClassNode.__init__];\n    process_class_func_n[process_class_func_node] --> FuncDefNode.__init__[FuncDefNode.__init__];\n    add_import[add_import] --> process_import_node[process_import_node];\n    add_import[add_import] --> process_from_import_[process_from_import_node];\n    add_call_or_import[add_call_or_import] --> process_import_node[process_import_node];\n    add_call_or_import[add_call_or_import] --> process_from_import_[process_from_import_node];\n    add_call_or_import[add_call_or_import] --> process_call_node[process_call_node];\n    walk_node_children[walk_node_children] --> ast.walk[ast.walk];\n    process_class_functi[process_class_function_def] --> process_func_def_nod[process_func_def_node];\n    process_class_functi[process_class_function_def] --> process_func_def_chi[process_func_def_children];\n    process_class_method[process_class_methods] --> process_class_functi[process_class_function_def];\n    get_instantiated_obj[get_instantiated_object_name] --> sibling_list.index[sibling_list.index];\n    update_call_data_for[update_call_data_for_object_info] --> get_instantiated_obj[get_instantiated_object_name];\n    process_func_def_chi[process_func_def_children] --> walk_node_children[walk_node_children];\n    process_func_def_chi[process_func_def_children] --> process_func_def_nod[process_func_def_node];\n    process_func_def_chi[process_func_def_children] --> add_import[add_import];\n    process_func_def_chi[process_func_def_children] --> process_call_node[process_call_node];\n    process_func_def_chi[process_func_def_children] --> update_call_data_for[update_call_data_for_object_info];\n    process_script_work[process_script_work] --> walk_node_children[walk_node_children];\n    process_script_work[process_script_work] --> process_call_node[process_call_node];\n    process_script_work[process_script_work] --> update_call_data_for[update_call_data_for_object_info];\n    parse_module_node[parse_module_node] --> process_class_method[process_class_methods];\n    parse_module_node[parse_module_node] --> process_class_node[process_class_node];\n    parse_module_node[parse_module_node] --> process_func_def_nod[process_func_def_node];\n    parse_module_node[parse_module_node] --> process_func_def_chi[process_func_def_children];\n    parse_module_node[parse_module_node] --> add_import[add_import];\n    parse_module_node[parse_module_node] --> process_script_work[process_script_work];\n    manage_module_import[manage_module_imports] --> collections.defaultd[collections.defaultdict];\n    manage_module_import[manage_module_imports] --> ImportNode.__init__[ImportNode.__init__];\n    get_walked_scripted_[get_walked_scripted_from_filename] --> pathlib.Path[pathlib.Path];\n    get_walked_scripted_[get_walked_scripted_from_filename] --> walk_script[walk_script];\n    get_top_level_node_f[get_top_level_node_from_filename] --> ast.parse[ast.parse];\n    get_top_level_node_f[get_top_level_node_from_filename] --> script_contents.read[script_contents.read];\n    extract_node_structu[extract_node_structure_from_script] --> pathlib.Path[pathlib.Path];\n    extract_node_structu[extract_node_structure_from_script] --> get_top_level_node_f[get_top_level_node_from_filename];\n    extract_node_structu[extract_node_structure_from_script] --> parse_module_node[parse_module_node];\n    extract_node_structu[extract_node_structure_from_script] --> manage_module_import[manage_module_imports];\nsubgraph ImportNode\n\nend\nsubgraph CallNode\n\nend\nsubgraph FuncDefNode\n\nend\nsubgraph ClassNode\n\nend\nsubgraph pathlib\n    pathlib.Path\nend\nsubgraph collections\n    collections.defaultd\nend\nsubgraph ast\n    ast.parse\n    ast.walk\nend\n\n\n\n\n\n\n\n\n\nyou can nicely see the recursive nature of the process where get_submodule_desc is pointing to itself\nthe class subgraphs for our data containers like ImportNode and ClassNode are empty. This is because we’re using @dataclass for simplicity. A potential improvement would be parsing them knowing that they have __init__ methods coming from the decorator\nwe can also see some dead code where process_class_func_node has nothing depending on it and we know we aren’t calling it\n\n\n\nCollections\nFinally, let’s look at one of the standard library modules. Given we use collections when building this crawler, that seems like a fun choice.\nOn my machine (I’m using WSL 2), the library is stored at /usr/lib/python3.8/collections. So, I extract the module by pointing to that directory, which has two files __init__ and abc. The latter is pretty sparse, because it’s just performing imports.\nIf we were to get the entire graph for collections it would actually be pretty horrendous. This is because it has many classes in it and each class performs a ton of calls. To give you the power to select a subgraph of the code information there are options for create_graph_description:\n\nwanted_classes - only display information about the classes whose names are in this list\ninclude_body_commands - include the calls that are made in the body of the script\ninclude_function_defs - include details about the module’s non-class function definitions\n\nFirst, we extract the information to create the module info:\ncollections_location = \"/usr/lib/python3.8/collections\"\nm_info = extract_code_information(directories=[collections_location])\nThen with [c.name for c in m_info[\"__init__\"][\"class_list\"]] we can see these are the classes in __init__.py:\n['_OrderedDictKeysView',\n '_OrderedDictItemsView',\n '_OrderedDictValuesView',\n '_Link',\n 'OrderedDict',\n 'Counter',\n 'ChainMap',\n 'UserDict',\n 'UserList',\n 'UserString']\nIf we only want information about OrderedDict then we use create_graph_description with these arguments to mute body calls and other function definitions:\ncreate_graph_description(\n    m_info[\"__init__\"],\n    wanted_classes=[\"OrderedDict\"],\n    include_body_commands=False,\n    include_function_defs=False\n)\n\n\n\n\ngraph LR;\n    OrderedDict.__init__[OrderedDict.__init__] --> _Link.__init__[_Link.__init__];\n    OrderedDict.__init__[OrderedDict.__init__] --> _proxy[_proxy];\n    OrderedDict.__init__[OrderedDict.__init__] --> self.__update[self.__update];\n    OrderedDict.__setite[OrderedDict.__setitem__] --> Link[Link];\n    OrderedDict.__setite[OrderedDict.__setitem__] --> proxy[proxy];\n    OrderedDict.__setite[OrderedDict.__setitem__] --> dict_setitem[dict_setitem];\n    OrderedDict.__delite[OrderedDict.__delitem__] --> dict_delitem[dict_delitem];\n    OrderedDict.__delite[OrderedDict.__delitem__] --> self.__map.pop[self.__map.pop];\n    OrderedDict.clear[OrderedDict.clear] --> self.__map.clear[self.__map.clear];\n    OrderedDict.clear[OrderedDict.clear] --> dict.clear[dict.clear];\n    OrderedDict.popitem[OrderedDict.popitem] --> dict.pop[dict.pop];\n    OrderedDict.__sizeof[OrderedDict.__sizeof__] --> sizeof[sizeof];\n    OrderedDict.__sizeof[OrderedDict.__sizeof__] --> sizeof[sizeof];\n    OrderedDict.__sizeof[OrderedDict.__sizeof__] --> sizeof[sizeof];\n    OrderedDict.__sizeof[OrderedDict.__sizeof__] --> sizeof[sizeof];\n    OrderedDict.keys[OrderedDict.keys] --> _OrderedDictKeysView[_OrderedDictKeysView.__init__];\n    OrderedDict.items[OrderedDict.items] --> _OrderedDictItemsVie[_OrderedDictItemsView.__init__];\n    OrderedDict.values[OrderedDict.values] --> _OrderedDictValuesVi[_OrderedDictValuesView.__init__];\n    OrderedDict.__reduce[OrderedDict.__reduce__] --> copy[copy];\n    OrderedDict.__reduce[OrderedDict.__reduce__] --> vars[vars];\n    OrderedDict.__reduce[OrderedDict.__reduce__] --> vars[vars];\n    OrderedDict.__reduce[OrderedDict.__reduce__] --> OrderedDict.__init__[OrderedDict.__init__];\n    OrderedDict.__reduce[OrderedDict.__reduce__] --> inst_dict.pop[inst_dict.pop];\n    OrderedDict.copy[OrderedDict.copy] --> self.__class__[self.__class__];\n    OrderedDict.__eq__[OrderedDict.__eq__] --> dict.__eq__[dict.__eq__];\n    OrderedDict.__eq__[OrderedDict.__eq__] --> dict.__eq__[dict.__eq__];\nsubgraph OrderedDict\n    OrderedDict.__init__\n    OrderedDict.__setite\n    OrderedDict.__delite\n    OrderedDict.__iter__\n    OrderedDict.__revers\n    OrderedDict.clear\n    OrderedDict.popitem\n    OrderedDict.move_to_\n    OrderedDict.__sizeof\n    OrderedDict.keys\n    OrderedDict.items\n    OrderedDict.values\n    OrderedDict.pop\n    OrderedDict.setdefau\n    OrderedDict.__repr__\n    OrderedDict.__reduce\n    OrderedDict.copy\n    OrderedDict.fromkeys\n    OrderedDict.__eq__\nend\nsubgraph _collections_abc\n    _collections_abc.Mut\nend\nsubgraph _sys\n    _sys._getframe\n    _sys.intern\nend\nsubgraph warnings\n    warnings.warn\nend\n\n\n\n\n\n\n\n\n\ndespite being a more complex example, the call depth is actually pretty shallow\nmany operations are defined in terms of operations of simpler data structures, in this case operations on regular dictionaries\nwe have some external module data here, from warnings, _sys, and _collections_abc. These aren’t used by OrderedDict, but are handled differently, so you would have to do a little more work to mute them.\n\nLet’s look at another class (Counter) but show any non-class functions and work done in the script. We do this using the fact that the defaults are True for showing that data:\ncreate_graph_description(m_info[\"__init__\"], wanted_classes=[\"Counter\"])\n\n\n\n\n\ngraph LR;\n    __getattr__[__getattr__] --> getattr[getattr];\n    __getattr__[__getattr__] --> warnings.warn[warnings.warn];\n    __getattr__[__getattr__] --> globals[globals];\n    __getattr__[__getattr__] --> AttributeError[AttributeError];\n    namedtuple[namedtuple] --> field_names.replace[field_names.replace];\n    namedtuple[namedtuple] --> _sys.intern[_sys.intern];\n    namedtuple[namedtuple] --> seen.add[seen.add];\n    namedtuple[namedtuple] --> _iskeyword[_iskeyword];\n    namedtuple[namedtuple] --> name.startswith[name.startswith];\n    namedtuple[namedtuple] --> name.isidentifier[name.isidentifier];\n    namedtuple[namedtuple] --> _iskeyword[_iskeyword];\n    namedtuple[namedtuple] --> name.isidentifier[name.isidentifier];\n    namedtuple[namedtuple] --> seen.add[seen.add];\n    namedtuple[namedtuple] --> name.startswith[name.startswith];\n    namedtuple[namedtuple] --> exec[exec];\n    namedtuple[namedtuple] --> tuple_new[tuple_new];\n    namedtuple[namedtuple] --> _len[_len];\n    namedtuple[namedtuple] --> self._make[self._make];\n    namedtuple[namedtuple] --> _map[_map];\n    namedtuple[namedtuple] --> _dict[_dict];\n    namedtuple[namedtuple] --> _zip[_zip];\n    namedtuple[namedtuple] --> _tuple[_tuple];\n    namedtuple[namedtuple] --> _sys.intern[_sys.intern];\n    namedtuple[namedtuple] --> _tuplegetter[_tuplegetter];\n    namedtuple[namedtuple] --> f_globals.get[f_globals.get];\n    namedtuple[namedtuple] --> _sys._getframe[_sys._getframe];\n    _count_elements[_count_elements] --> mapping_get[mapping_get];\n    Counter.__init__[Counter.__init__] --> __init__[__init__];\n    Counter.__init__[Counter.__init__] --> super[super];\n    Counter.__init__[Counter.__init__] --> self.update[self.update];\n    Counter.most_common[Counter.most_common] --> sorted[sorted];\n    Counter.most_common[Counter.most_common] --> _itemgetter[_itemgetter];\n    Counter.most_common[Counter.most_common] --> _heapq.nlargest[_heapq.nlargest];\n    Counter.most_common[Counter.most_common] --> _itemgetter[_itemgetter];\n    Counter.elements[Counter.elements] --> _chain.from_iterable[_chain.from_iterable];\n    Counter.elements[Counter.elements] --> _starmap[_starmap];\n    Counter.fromkeys[Counter.fromkeys] --> NotImplementedError[NotImplementedError];\n    Counter.update[Counter.update] --> _count_elements[_count_elements];\n    Counter.update[Counter.update] --> update[update];\n    Counter.update[Counter.update] --> self_get[self_get];\n    Counter.update[Counter.update] --> super[super];\n    Counter.update[Counter.update] --> self.update[self.update];\n    Counter.subtract[Counter.subtract] --> self_get[self_get];\n    Counter.subtract[Counter.subtract] --> self_get[self_get];\n    Counter.subtract[Counter.subtract] --> self.subtract[self.subtract];\n    Counter.copy[Counter.copy] --> self.__class__[self.__class__];\n    Counter.__delitem__[Counter.__delitem__] --> __delitem__[__delitem__];\n    Counter.__delitem__[Counter.__delitem__] --> super[super];\n    Counter.__repr__[Counter.__repr__] --> format[format];\n    Counter.__repr__[Counter.__repr__] --> self.most_common[self.most_common];\n    Counter.__add__[Counter.__add__] --> Counter.__init__[Counter.__init__];\n    Counter.__sub__[Counter.__sub__] --> Counter.__init__[Counter.__init__];\n    Counter.__or__[Counter.__or__] --> Counter.__init__[Counter.__init__];\n    Counter.__and__[Counter.__and__] --> Counter.__init__[Counter.__init__];\n    Counter.__pos__[Counter.__pos__] --> Counter.__init__[Counter.__init__];\n    Counter.__neg__[Counter.__neg__] --> Counter.__init__[Counter.__init__];\n    Counter.__iadd__[Counter.__iadd__] --> self._keep_positive[self._keep_positive];\n    Counter.__isub__[Counter.__isub__] --> self._keep_positive[self._keep_positive];\n    Counter.__ior__[Counter.__ior__] --> self._keep_positive[self._keep_positive];\n    Counter.__iand__[Counter.__iand__] --> self._keep_positive[self._keep_positive];\n    main[main] --> _collections_abc.Mut[_collections_abc.MutableSequence.register];\n    main[main] --> property[property];\n    main[main] --> _itemgetter[_itemgetter];\n    main[main] --> getattr[getattr];\n    main[main] --> warnings.warn[warnings.warn];\n    main[main] --> globals[globals];\n    main[main] --> AttributeError[AttributeError];\n    main[main] --> field_names.replace[field_names.replace];\n    main[main] --> _sys.intern[_sys.intern];\n    main[main] --> seen.add[seen.add];\n    main[main] --> _iskeyword[_iskeyword];\n    main[main] --> name.startswith[name.startswith];\n    main[main] --> name.isidentifier[name.isidentifier];\n    main[main] --> _iskeyword[_iskeyword];\n    main[main] --> name.isidentifier[name.isidentifier];\n    main[main] --> seen.add[seen.add];\n    main[main] --> name.startswith[name.startswith];\n    main[main] --> exec[exec];\n    main[main] --> tuple_new[tuple_new];\n    main[main] --> _len[_len];\n    main[main] --> self._make[self._make];\n    main[main] --> _map[_map];\n    main[main] --> _dict[_dict];\n    main[main] --> _zip[_zip];\n    main[main] --> _tuple[_tuple];\n    main[main] --> _sys.intern[_sys.intern];\n    main[main] --> _tuplegetter[_tuplegetter];\n    main[main] --> f_globals.get[f_globals.get];\n    main[main] --> _sys._getframe[_sys._getframe];\n    main[main] --> mapping_get[mapping_get];\n    main[main] --> getattr[getattr];\n    main[main] --> warnings.warn[warnings.warn];\n    main[main] --> globals[globals];\n    main[main] --> AttributeError[AttributeError];\n    main[main] --> field_names.replace[field_names.replace];\n    main[main] --> _sys.intern[_sys.intern];\n    main[main] --> seen.add[seen.add];\n    main[main] --> _iskeyword[_iskeyword];\n    main[main] --> name.startswith[name.startswith];\n    main[main] --> name.isidentifier[name.isidentifier];\n    main[main] --> _iskeyword[_iskeyword];\n    main[main] --> name.isidentifier[name.isidentifier];\n    main[main] --> seen.add[seen.add];\n    main[main] --> name.startswith[name.startswith];\n    main[main] --> exec[exec];\n    main[main] --> tuple_new[tuple_new];\n    main[main] --> _len[_len];\n    main[main] --> self._make[self._make];\n    main[main] --> _map[_map];\n    main[main] --> _dict[_dict];\n    main[main] --> _zip[_zip];\n    main[main] --> _tuple[_tuple];\n    main[main] --> _sys.intern[_sys.intern];\n    main[main] --> _tuplegetter[_tuplegetter];\n    main[main] --> f_globals.get[f_globals.get];\n    main[main] --> _sys._getframe[_sys._getframe];\n    main[main] --> mapping_get[mapping_get];\nsubgraph Counter\n    Counter.__init__\n    Counter.__missing__\n    Counter.most_common\n    Counter.elements\n    Counter.fromkeys\n    Counter.update\n    Counter.subtract\n    Counter.copy\n    Counter.__reduce__\n    Counter.__delitem__\n    Counter.__repr__\n    Counter.__add__\n    Counter.__sub__\n    Counter.__or__\n    Counter.__and__\n    Counter.__pos__\n    Counter.__neg__\n    Counter._keep_positi\n    Counter.__iadd__\n    Counter.__isub__\n    Counter.__ior__\n    Counter.__iand__\nend\nsubgraph _collections_abc\n    _collections_abc.Mut\nend\nsubgraph _sys\n    _sys._getframe\n    _sys.intern\nend\nsubgraph warnings\n    warnings.warn\nend\n\n\n\n\n\n\n\n\nWe can see that there is a lot going on, but also a lot of common boiler plate used. When you are inspecting more complicated module likes this, you’ll want to use the options provided to break it into more readable chunks."
  },
  {
    "objectID": "posts/graph/Graph_My_Code_2.html#pygraphcrawler---code-graph-creation",
    "href": "posts/graph/Graph_My_Code_2.html#pygraphcrawler---code-graph-creation",
    "title": "Graph My Code 2: Visualizing Function Dependencies in Python",
    "section": "pygraphcrawler - Code Graph Creation",
    "text": "pygraphcrawler - Code Graph Creation\nThe final repo is here. The main parts are\n\ncode_extraction.py - entry point script to coordinate which files to parse and run the parser\ndep_parser.py - generate module info by parsing a module using ast\ncode_graph.py - convert module info into graph edge data\nviz_code.py - use edge data and module info to create Mermaid graph descriptions.\n\n\nCrawl Code\nWe start by parsing the full script and look at the top-level module node. This lets us know the name of the script and gives us its immediate children in the ast graph.\nmodule_node = get_top_level_node_from_filename(path)\n\nimport_list, call_list, func_defs, class_list = parse_module_node(\n    module_node, current_module_name, verbose=verbose\n)\nThis work is done in parse_module_node, where we go through each node in the body of the module_node and depending on the node type, process it and its child ast nodes. This allows us to distinguish calls to functions that are made inside function definitions from calls that are made in the general script, as well as annotate those dependencies. By walking these parsed elements, we can extract the data stored in an ast node and convert it to more convenient classes. For instance rather than working with ast.Call objects we use a custom CallNode object:\n@dataclass\nclass CallNode:\n    module: str  # what module does the called function belong to\n    name: str  # function name\n    call_lineno: int  # where was the call\n    called_by: str = None  # what was the caller\nThen, provided an ast.Call node, we parse it for that data above like this:\nfunc_data = node.func  # func is an attribute of an ast.Call object\n\n# func_data can either be an Attribute or a Name, which require\n# different ways to access their properties\nif isinstance(func_data, ast.Attribute):\n    function_name = func_data.attr\n    value = func_data.value\n    # handle submodule calls like np.linalg.norm\n    submodule_desc = get_submodule_desc(value)  \n    submodule_desc.reverse()\n\n    call_node = CallNode(\n        module=submodule_desc, name=function_name, call_lineno=node.lineno\n    )\nelif isinstance(func_data, ast.Name):\n    call_node = CallNode(\n        name=func_data.id,\n        module=[],  # the call node did not have this detail, requiring separate handling\n        call_lineno=node.lineno,\n    )\nelse:\n    print(\"error\")\n    print(node)\n    return None\nBy parsing it, we have an easier way to reference the call and its properties. We have similar classes and parsers for the other node types.\nOnce we have the module data we collect it into a dictionary specifying:\n\nimport_list - which modules were imported and where did we call them\ncall_list - all function calls\nfunc_defs - function definitions\nclass_list - class data including methods\n\nFrom here, we can take this parsed module data and convert it to edges. This is where we can prune some of the lower level function calls (like print()) that we are not interested in visualizing. We also can compress multiple calls from one function to another into a single edge with a weight tracking the call count (as in the first example).\nFinally, we take the edges and module data and create the Mermaid graph description:\n\nedge data allows us to specify the graph edges and carefully check naming conventions\nmodule data lets us create subgraphs to group calls into the same external module. Grouping calls in this way makes dependencies a lot easier to understand.\nmodule data also enables grouping class methods"
  },
  {
    "objectID": "posts/graph/Graph_My_Code_2.html#uses",
    "href": "posts/graph/Graph_My_Code_2.html#uses",
    "title": "Graph My Code 2: Visualizing Function Dependencies in Python",
    "section": "Uses",
    "text": "Uses\n\nCode Analysis\nAs we saw in the examples, the code graph can tell us some basic things at a glance:\n\nwhat are the top level functions - look at the nodes farthest to the left\nwhat external libraries do we have the most dependencies on - look at call counts into the module subgraphs and terminal nodes with many ancestors\ndo we have dead code - is there a component of the graph that contains a top level function we no longer use?\n\n\n\nDocumentation\nFor smaller modules, this is also a form of documentation. You can get a pretty good idea of what a function is doing by looking at its name and what it calls. For larger modules, this still applies but something with many definitions can become cluttered and you may want to break it into pieces."
  },
  {
    "objectID": "posts/graph/Graph_My_Code_2.html#extensions",
    "href": "posts/graph/Graph_My_Code_2.html#extensions",
    "title": "Graph My Code 2: Visualizing Function Dependencies in Python",
    "section": "Extensions",
    "text": "Extensions\n\nCode Similarity\n\nGiven two code graphs, how can we measure similarity?\n\n\nuse basic graph similarity measures from a graph object constructed from the edge data\nuse the calls into external modules and see how much they overlap\nuse the markdown graph as a compressed version of two modules and run text similarity methods, which can be difficult to run on the non-compressed original source\n\nBy identifying when we have similar code structures, we can begin to develop better abstractions of code (as needed). Right now, this only generates the graph data, but this would be a possible extension of that data.\n\n\nCode Generation\n\nUsing the basic building blocks of an answer to a small problem, can we randomly recreate it?\n\nThis is more of a fringe use case, but you could use the inter-function dependency data to shrink the space of grammatical code expressions in order to randomly generate code. Not quite a copilot by any stretch, but could generate some interesting results."
  },
  {
    "objectID": "posts/graph/Graph_My_Code_2.html#lessons-learned",
    "href": "posts/graph/Graph_My_Code_2.html#lessons-learned",
    "title": "Graph My Code 2: Visualizing Function Dependencies in Python",
    "section": "Lessons Learned",
    "text": "Lessons Learned\n\nWhen to Walk, When to Run\nInitially, I was parsing code using the full list of nodes generated by ast.walk(). Unfortunately, this makes it difficult to connect calls to the function definitions they may reside in. In the walked list, you can get a function definition node followed by call nodes in the list, but it then requires more work to determine if those calls were in the function definition or outside of it just later in the walked tree. You could get around this with line number comparisons (I leave some deprecated functions in the repo for doing this), but I found it simpler to recursively use the node structure generated by ast.\nOn the other hand, you sometimes do want to sweep through all the child nodes. When you manually walk the tree, you can end up with heavily nested structure to call. For instance, f() + (2 * (g() + h())) involves going through the expression and parsing binary operations and nested expressions just to get the data that you care about (the three functions that were called).\nWhen you crawl the node structure, selectively using ast.walk on some of the child nodes of the top level module node can avoid this. Get the highest level nodes – like classes and function definitions – and then allow ast to quickly walk all the children of those nodes so you can get all the data you care about and also tie it to those higher level nodes.\n\n\nKeywords and Meta-programming\nIn my graph description generator, I’m careful to rename words like “map” and “find” when they are used outside of node labels. This is because these appear to be keywords of the process of generating the graph image from the markdown, so including them as regular names of nodes will cause the preview to error out on VSCode. There was also a special case for the word “end” appearing as part of a node name because this conflicts with using “end” to delineate subgraphs, but the fix is to capitalize part of “end”. This is why it is important to have input sanitation performed before compiling the final product.\nThis is a general concern to keep in mind, when you’re creating descriptions of code you want to be careful about escaping problematic terms so that whatever system you are using does not confuse a command with a description. I found similar issues when printing out the Mermaid graphs in a notebook, sometimes causing the entire notebook to hit some invalid states. Hard to issue good guidance other than “be on the lookout.”\n\n\nLow-level Calls\nIn common expressions like some_list.append(item) you are performing a function call. While it may be useful to see this in the final graph, I found that it cluttered the visualization. To get around this, I kept such things in the parsed data, but use explicit lists of common function names to exclude when creating the graph markdown. Along with some rules for how to handle parsing a call’s module, this removed a lot of noise.\nThis is an opportunity for improvement though, because I’m using simple rules to determine when to skip over calls. Ideally using something to determine the type of object a function is attached to (are we running an operation on a dictionary or a list) would improve this.\n\n\nClasses\nRelated to difficulty in detecting type information, when you have class definitions involved things begin to get tricky. In fact, I thought I was done with the project before I saw how large of a gap effectively processing classes was.\nGetting the function definition information out of class methods is straightforward, because things are nicely grouped in an ast node for the class. Less obvious is how to determine when you are making a call to an instantiated object of a class.\nMy solution was a hack: when you have a call, check if the name overlaps with any of the names of classes in the module. If so, it’s likely creating an object, so jump up the walked syntax tree to the assignment and keep track of the name used for the assignment target (the x in x = 1). Then in that scope you know any calls involving that object name are calls to the class methods and you can track it appropriately.\nTo do this “the right way” you would have to almost run your own version of the interpreter, as mentioned in the low-level calls lesson. This would definitely be an interesting problem to work on, but I’m satisfied omitting the ability to parse more elaborate Python code in this visualizer."
  },
  {
    "objectID": "posts/graph/Graph_My_Code_2.html#have-fun",
    "href": "posts/graph/Graph_My_Code_2.html#have-fun",
    "title": "Graph My Code 2: Visualizing Function Dependencies in Python",
    "section": "Have Fun",
    "text": "Have Fun\nFeel free to experiment with the code and let me know if you find other use cases or if you’d like to contribute."
  },
  {
    "objectID": "posts/graph/Graph_My_Code_1.html",
    "href": "posts/graph/Graph_My_Code_1.html",
    "title": "Graph My Code 1: Creating a Graph of Function Dependencies in Python",
    "section": "",
    "text": "Using Python’s ast to build graph visualizations of Python code."
  },
  {
    "objectID": "posts/graph/Graph_My_Code_1.html#intro",
    "href": "posts/graph/Graph_My_Code_1.html#intro",
    "title": "Graph My Code 1: Creating a Graph of Function Dependencies in Python",
    "section": "Intro",
    "text": "Intro\nIn the previous entry, we found connections between different Python scripts manually. We iterated through lines of code with a simple check for lines that began with something like from x import or import x. This works fine for getting an idea of module dependencies, but if we begin to ask questions like “what functions are imported” and “what are our dependencies per function” then the complexity begins to increase.\nFor example, if we want to track which functions are called in a script (how often did I use a deprecated function that needs to be replaced in the next version?) we would have to:\n\ntake note of which functions are called in from the initial imports from x import y, z\ngo through the rest of the code line by line and find when y and z are used\nfind when functions of a library are called using notation like x.func1()\n\nIf we also want to track the dependencies of each of our functions independently (rather than for the entire script) then this would involve systematically tracking when you are inside of a function definition.\nAt some point in going through code, keeping a careful log of what has been defined, and differentiating between being in a function definition or in a comment, you’re building a parser. This is fun, but it can be difficult to do correctly. Luckily, Python has a built-in parser you can use: ast for Abstract Syntax Trees. This will build a syntax tree of Python code and allow us to crawl it to find the function dependencies we’re looking for.\nGoals:\n\nexplore the ast library\nuse it to find function dependencies in code\nvisualize these dependencies with a graph"
  },
  {
    "objectID": "posts/graph/Graph_My_Code_1.html#explore-ast",
    "href": "posts/graph/Graph_My_Code_1.html#explore-ast",
    "title": "Graph My Code 1: Creating a Graph of Function Dependencies in Python",
    "section": "Explore ast",
    "text": "Explore ast\nFirst, here’s what Python version I’m using (syntax can change from version to version, so this may not work exactly the same):\n\n!python3 -V\n\nPython 3.8.10\n\n\n\nimport ast\n\nHere is some example code:\n\n!cat example/main.py\n\nfrom worker import do_stuff\n\ndo_stuff()\n\n\n\nParse\nTo use ast, begin by reading in the content of a file and then use ast.parse:\n\nexample_script = open(\"example/main.py\", \"r\")\nparsed_script = ast.parse(example_script.read())\n\nThis produces a nested syntax tree. Here the entire module is the top level:\n\nparsed_script\n\n<_ast.Module at 0x7ffb14597430>\n\n\nThen its .body has the contents, and import line and some expression:\n\nparsed_script.body\n\n[<_ast.ImportFrom at 0x7ffb1459ce80>, <_ast.Expr at 0x7ffb1454f430>]\n\n\n\n\nWalk\nThe nested elements keeps going until you hit the lowest levels of names of variables. You can see the full collection of elements with .walk to walk all paths of the syntax tree:\n\nwalked_script = ast.walk(parsed_script)\nw_l = list(walked_script)\n\nThese are all the nodes:\n\nw_l\n\n[<_ast.Module at 0x7ffb14597430>,\n <_ast.ImportFrom at 0x7ffb1459ce80>,\n <_ast.Expr at 0x7ffb1454f430>,\n <_ast.alias at 0x7ffb1454f340>,\n <_ast.Call at 0x7ffb1454f550>,\n <_ast.Name at 0x7ffb1454f580>,\n <_ast.Load at 0x7ffb1815c0d0>]\n\n\nSome important ones for our purposes are Module, ImportFrom, and Call.\n\n\nModule Node\nFirst is a node for a Python module that we saw at the top level of the object returned from .parse:\n\nmodule_node = w_l[0]\n\n\nmodule_node.body\n\n[<_ast.ImportFrom at 0x7ffb1459ce80>, <_ast.Expr at 0x7ffb1454f430>]\n\n\n\n\nImportFrom Node\nThe ImportFrom node is specifically for imports structured like from x import y.\n\nimport_from_node = w_l[1]\n\nWe can get the module name\n\nimport_from_node.module\n\n'worker'\n\n\nand a list of the imported functions\n\nimport_from_node.names\n\n[<_ast.alias at 0x7ffb1454f340>]\n\n\nwhose names we can access with the .name attribute\n\nimport_from_node.names[0].name\n\n'do_stuff'\n\n\nThis tells us that we imported do_stuff from worker, even if it doesn’t specifically tell us the context of it (where did we use the function if at all).\n\n\nCall Nodes\nFunction calls are in Call nodes:\n\ncall_node = w_l[-3]\ncall_node\n\n<_ast.Call at 0x7ffb1454f550>\n\n\nWe can get the arguments of the function, the line number where the call was made, and the name of the function being called:\n\ncall_node.args\n\n[]\n\n\n\ncall_node.lineno\n\n3\n\n\n\ncall_node.func.id\n\n'do_stuff'"
  },
  {
    "objectID": "posts/graph/Graph_My_Code_1.html#start-crawling-code",
    "href": "posts/graph/Graph_My_Code_1.html#start-crawling-code",
    "title": "Graph My Code 1: Creating a Graph of Function Dependencies in Python",
    "section": "Start Crawling Code",
    "text": "Start Crawling Code\nUse a function to generate a walked list of AST elements given a script name:\n\ndef walk_script(filename):\n    worker_script = open(filename, \"r\")\n    parsed_worker = ast.parse(worker_script.read())\n    walked_worker = ast.walk(parsed_worker)\n    work_w = list(walked_worker)\n    return work_w\n\n\nwork_w = walk_script(\"example/worker.py\")\n\n\n!cat example/worker.py\n\nfrom helper_1 import greeting\nfrom helper_2 import name\n\ndef do_stuff():\n    print(f\"{greeting()}, {name()}!\")\n\n\n\nwork_w\n\n[<_ast.Module at 0x7ffb144f1c70>,\n <_ast.ImportFrom at 0x7ffb144f1fd0>,\n <_ast.ImportFrom at 0x7ffb144f7310>,\n <_ast.FunctionDef at 0x7ffb144f73a0>,\n <_ast.alias at 0x7ffb144f7520>,\n <_ast.alias at 0x7ffb144f74f0>,\n <_ast.arguments at 0x7ffb144f73d0>,\n <_ast.Expr at 0x7ffb144f7370>,\n <_ast.Call at 0x7ffb144f71c0>,\n <_ast.Name at 0x7ffb144f7160>,\n <_ast.JoinedStr at 0x7ffb144f7400>,\n <_ast.Load at 0x7ffb1815c0d0>,\n <_ast.FormattedValue at 0x7ffb144f7430>,\n <_ast.Constant at 0x7ffb144f7490>,\n <_ast.FormattedValue at 0x7ffb144f7550>,\n <_ast.Constant at 0x7ffb144f75e0>,\n <_ast.Call at 0x7ffb144f72e0>,\n <_ast.Call at 0x7ffb144f7580>,\n <_ast.Name at 0x7ffb144f7460>,\n <_ast.Name at 0x7ffb144f75b0>,\n <_ast.Load at 0x7ffb1815c0d0>,\n <_ast.Load at 0x7ffb1815c0d0>]\n\n\nIf we start at the top, the module node tells us this code consists of two import lines and a function definition:\n\nwork_w[0]\n\n<_ast.Module at 0x7ffb144f1c70>\n\n\n\nwork_w[0].body\n\n[<_ast.ImportFrom at 0x7ffb144f1fd0>,\n <_ast.ImportFrom at 0x7ffb144f7310>,\n <_ast.FunctionDef at 0x7ffb144f73a0>]\n\n\nThe names of the imported modules:\n\nwork_w[0].body[0].module\n\n'helper_1'\n\n\n\nwork_w[0].body[1].module\n\n'helper_2'\n\n\nWe can begin to see how to parse for module and function dependency information. We can used the walked syntax tree and record information about module imports, function calls, and function definitions. Note, if we are crawling the children of FunctionDef then the function being defined has those children as dependencies.\nPart of the power of ast is that our dependency parser is more robust to code that is less well-formatted.\n\n!cat example/worker_difficult.py\n\nfrom helper_1 import greeting\n\ndef do_stuff():\n    print(f\"{greeting()}, {name()}!\")\n\nfrom helper_2 import name\n\n\n\nwork_w = walk_script(\"example/worker_difficult.py\")\n\nThe only thing that changes is the order in the body of the module node, the second import is now after the function definition (but not a child of it) and this information is preserved:\n\nwork_w[0].body\n\n[<_ast.ImportFrom at 0x7ffb14505730>,\n <_ast.FunctionDef at 0x7ffb14505700>,\n <_ast.ImportFrom at 0x7ffb14505c70>]"
  },
  {
    "objectID": "posts/graph/Graph_My_Code_1.html#constructing-the-dependency-parser",
    "href": "posts/graph/Graph_My_Code_1.html#constructing-the-dependency-parser",
    "title": "Graph My Code 1: Creating a Graph of Function Dependencies in Python",
    "section": "Constructing the Dependency Parser",
    "text": "Constructing the Dependency Parser\nAt the end of this article, there are details of more complex parsing cases, but let’s jump into using the parser:\n\ncreate the syntax tree,\nwalk the nodes,\nperform a type check on each node and\nextract the information from the node depending on the type\n\n\n!cat example/abyss.py\n\nimport numpy as np\n\nz = np.zeroes(5)\n\ndef mul():\n    a = np.array([[1, 0],\n                  [0, 1]])\n    b = np.array([[4, 1],\n                  [2, 2]])\n    return np.matmul(a, b)\n\ndef eigs_of_product():\n    a = np.array([[1, 0],\n                  [0, 1]])\n    b = np.array([[4, 1],\n                  [2, 2]])\n    product = np.matmul(a, b)\n    eigs = np.linalg.eigs(product)\n    np.linalg.debug.depth.error_print(eigs)  # this is a fake call, comment out if you want to run this\n    return eigs\n\n\n\ndef get_submodule_desc(value):\n    # this helps us handle submodules like the call of `np.linalg.eig` which is using the `linalg` submodule of `numpy`\n    module_call = []\n    # the submodules and functions will have different types\n    if isinstance(value, ast.Attribute):\n        module_call.append(value.attr)\n        module_call.extend(get_submodule_desc(value.value))  # since this was a submodule, keep parsing\n    elif isinstance(value, ast.Name):\n        module_call.append(value.id)  # finally hit the function\n    return module_call\n\nwork_w = walk_script(\"example/abyss.py\")  # 1. create the syntax tree and 2. walk the nodes\ncalls = [n for n in work_w if isinstance(n, ast.Call)]  # 3. perform a type check (to only get function calls)\nfor call in calls:\n    # 4. extract information from each call node\n    function = call.func\n    function_name = function.attr\n    value = function.value\n    submodule_desc = get_submodule_desc(value)\n    submodule_desc.reverse()  # we ended up parsing backwards\n    print(function_name, \".\".join(submodule_desc))  # returning the function name and parent module\n\nzeroes np\narray np\narray np\nmatmul np\narray np\narray np\nmatmul np\neigs np.linalg\nerror_print np.linalg.debug.depth\n\n\nSo we have a list of functions called in the script along with their home modules."
  },
  {
    "objectID": "posts/graph/Graph_My_Code_1.html#extract-code-info",
    "href": "posts/graph/Graph_My_Code_1.html#extract-code-info",
    "title": "Graph My Code 1: Creating a Graph of Function Dependencies in Python",
    "section": "Extract Code Info",
    "text": "Extract Code Info\nDo something like the above for each .py file in a directory you want to map out.\n\nfrom code_extraction import *\n\nWe have a simple utility to get all of the Python filenames from a collection of directories, along with separately specified filenames.\n\nget_all_filenames(directories=[\"example\", \"example2\"], other_python_filenames=[\"test.py\"])\n\n[PosixPath('example/main.py'),\n PosixPath('example/helper_2.py'),\n PosixPath('example/worker_difficult.py'),\n PosixPath('example/worker.py'),\n PosixPath('example/worker_more_difficult.py'),\n PosixPath('example/helper_1.py'),\n PosixPath('example/abyss.py'),\n PosixPath('example/sub/one.py'),\n PosixPath('example/sub/subsub/two.py'),\n PosixPath('example2/lo.py'),\n PosixPath('test.py')]\n\n\nWe’ll do a deeper dive in the next article, but we can create a module information dictionary by parsing and extracting information from each script.\n\nm_info = extract_code_information(directories=[\"example\", \"example2\"], other_python_filenames=[\"test.py\"])\n\n\nfrom pprint import pprint\n\nThe module names:\n\nm_info.keys()\n\ndict_keys(['main', 'helper_2', 'worker_difficult', 'worker', 'worker_more_difficult', 'helper_1', 'abyss', 'one', 'two', 'lo', 'test'])\n\n\nFor each module, we provide the list of imports, function calls, and function definitions:\n\nm_info[\"worker\"].keys()\n\ndict_keys(['import_list', 'call_list', 'func_defs'])\n\n\nThen our extracted information looks something like this:\n\npprint(m_info[\"worker\"])\n\n{'call_list': [CallNode(module=[], name='print', call_lineno=5, called_by='do_stuff'),\n               CallNode(module=[], name='greeting', call_lineno=5, called_by='do_stuff'),\n               CallNode(module=[], name='name', call_lineno=5, called_by='do_stuff')],\n 'func_defs': [FuncDefNode(name='do_stuff', module='worker', start_lineno=4, end_lineno=5, calls=[CallNode(module=[], name='print', call_lineno=5, called_by='do_stuff'), CallNode(module=[], name='greeting', call_lineno=5, called_by='do_stuff'), CallNode(module=[], name='name', call_lineno=5, called_by='do_stuff')])],\n 'import_list': [ImportNode(module='helper_1', function_names=['greeting'], level=0, alias=''),\n                 ImportNode(module='helper_2', function_names=['name'], level=0, alias='')]}\n\n\nWe have the function calls that are made, tracked by where they are called. We also have a log of the imported modules as well as the functions defined in a module."
  },
  {
    "objectID": "posts/graph/Graph_My_Code_1.html#construct-code-graph",
    "href": "posts/graph/Graph_My_Code_1.html#construct-code-graph",
    "title": "Graph My Code 1: Creating a Graph of Function Dependencies in Python",
    "section": "Construct Code Graph",
    "text": "Construct Code Graph\nThe module info dictionary m_info contains all the information we need to build up a code graph.\n\nfrom code_graph import *\n\nFor each function we can use the call list to create from functions to their parent modules:\n\nm_info[\"abyss\"][\"call_list\"]\n\n[CallNode(module=['np'], name='zeroes', call_lineno=3, called_by=None),\n CallNode(module=['np'], name='array', call_lineno=6, called_by='mul'),\n CallNode(module=['np'], name='array', call_lineno=8, called_by='mul'),\n CallNode(module=['np'], name='matmul', call_lineno=10, called_by='mul'),\n CallNode(module=['np'], name='array', call_lineno=13, called_by='eigs_of_product'),\n CallNode(module=['np'], name='array', call_lineno=15, called_by='eigs_of_product'),\n CallNode(module=['np'], name='matmul', call_lineno=17, called_by='eigs_of_product'),\n CallNode(module=['np', 'linalg'], name='eigs', call_lineno=18, called_by='eigs_of_product'),\n CallNode(module=['np', 'linalg', 'debug', 'depth'], name='error_print', call_lineno=19, called_by='eigs_of_product')]\n\n\n\ncreate_function_call_edges_simple(m_info[\"abyss\"])\n\n[('mul', 'np.array'),\n ('mul', 'np.array'),\n ('mul', 'np.matmul'),\n ('eigs_of_product', 'np.array'),\n ('eigs_of_product', 'np.array'),\n ('eigs_of_product', 'np.matmul'),\n ('eigs_of_product', 'np.linalg.eigs'),\n ('eigs_of_product', 'np.linalg.debug.depth.error_print')]\n\n\nWe can see that some functions are called multiple times in the same definition, so we compress this information while keeping track of the number of calls:\n\ncreate_function_call_edges(m_info[\"abyss\"])\n\n[('mul', 'np.array', 2),\n ('mul', 'np.matmul', 1),\n ('eigs_of_product', 'np.array', 2),\n ('eigs_of_product', 'np.matmul', 1),\n ('eigs_of_product', 'np.linalg.eigs', 1),\n ('eigs_of_product', 'np.linalg.debug.depth.error_print', 1)]"
  },
  {
    "objectID": "posts/graph/Graph_My_Code_1.html#visualize",
    "href": "posts/graph/Graph_My_Code_1.html#visualize",
    "title": "Graph My Code 1: Creating a Graph of Function Dependencies in Python",
    "section": "Visualize",
    "text": "Visualize\nFinally, we create mermaid graph descriptions of code.\n\nfrom viz_code import generate_desc\n\nNote, had to comment these out because my site generator became very confused about these markdown prints!\n\n# print(generate_desc(create_function_call_edges_simple(m_info[\"worker\"])))\n# ```{mermaid}\n#graph LR;\n#   do_stuff[do_stuff] --> print[print];\n#   do_stuff[do_stuff] --> greeting[greeting];\n#   do_stuff[do_stuff] --> name[name];\n# ```\n\n\n\n\n\ngraph LR;\n    do_stuff[do_stuff] --> print[print];\n    do_stuff[do_stuff] --> greeting[greeting];\n    do_stuff[do_stuff] --> name[name];\n\n\n\n\n\n\n\n\n\n# print(generate_desc(create_function_call_edges_simple(m_info[\"abyss\"])))\n\n\n\n\n\ngraph LR;\n    mul[mul] --> np.array[np.array];\n    mul[mul] --> np.array[np.array];\n    mul[mul] --> np.matmul[np.matmul];\n    eigs_of_product[eigs_of_product] --> np.array[np.array];\n    eigs_of_product[eigs_of_product] --> np.array[np.array];\n    eigs_of_product[eigs_of_product] --> np.matmul[np.matmul];\n    eigs_of_product[eigs_of_product] --> np.linalg.eigs[np.linalg.eigs];\n    eigs_of_product[eigs_of_product] --> np.linalg.debug.dept[np.linalg.debug.depth.error_print];\n\n\n\n\n\n\n\n\nIn the next installment we’ll go into the details about how to actually create this parser with a final repo for creating a function dependency graph."
  },
  {
    "objectID": "posts/graph/Graph_My_Code_1.html#addendum-more-complex-parsing",
    "href": "posts/graph/Graph_My_Code_1.html#addendum-more-complex-parsing",
    "title": "Graph My Code 1: Creating a Graph of Function Dependencies in Python",
    "section": "Addendum: More Complex Parsing",
    "text": "Addendum: More Complex Parsing\nThere are many complex cases that need to be handled to perform a thorough analysis. For instance, when the imports are not all tidily placed at the top and also:\n\nimporting multiple functions: from x import (a, b, c)\ninline imports: sometimes you only want to perform an import when you are using a specific function\naliasing: import numpy as np means we have to track np in the code\n\n\nwork_w = walk_script(\"example/worker_more_difficult.py\")\n\nHere we have examples of all the above:\n\n!cat example/worker_more_difficult.py\n\nfrom helper_1 import greeting\nfrom helper_3 import (a, b, c, d)\nfrom collections import *\n\ndef do_stuff():\n    import re\n    import talk as tk\n    print(f\"{greeting()}, {name()}!\")\n\nfrom helper_2 import name, crab\nfrom ..test import say_hello\nimport sub.one\nimport sub.subsub.two\nfrom sub.one import cat\nfrom sub.subsub.two import dog\n\n\nwe catch the imports outside of function definitions as before\n\nmodule_node = work_w[0]\nmodule_node.body\n\n[<_ast.ImportFrom at 0x7ffb1452b670>,\n <_ast.ImportFrom at 0x7ffb1452bf40>,\n <_ast.ImportFrom at 0x7ffb1452ba00>,\n <_ast.FunctionDef at 0x7ffb144b5040>,\n <_ast.ImportFrom at 0x7ffb144b5520>,\n <_ast.ImportFrom at 0x7ffb144b55e0>,\n <_ast.Import at 0x7ffb144b5670>,\n <_ast.Import at 0x7ffb144b5730>,\n <_ast.ImportFrom at 0x7ffb144b57f0>,\n <_ast.ImportFrom at 0x7ffb144b5880>]\n\n\n\nmodule_node.body[0].module\n\n'helper_1'\n\n\n\nmodule_node.body[0].names[0].name\n\n'greeting'\n\n\nWhen we have more than one function being imported:\n\nmodule_node.body[1].module\n\n'helper_3'\n\n\n\n[n.name for n in module_node.body[1].names]\n\n['a', 'b', 'c', 'd']\n\n\nImporting everything from a module\n\nmodule_node.body[2].module\n\n'collections'\n\n\nWe can see this is going to be a problem for us to solve. We have no real way of logging the functions we’re bringing in without parsing the entire module:\n\nmodule_node.body[2].names[0].name\n\n'*'\n\n\nWe can use the .level attribute to determine that something was a relative import (from ..test import say_hello):\n\nwork_w[0].body[5].module\n\n'test'\n\n\nlevel is an integer holding the level of the relative import (0 means absolute import)\n\nwork_w[0].body[5].level\n\n2\n\n\n\nSubdirectory Imports and the Import Node\nThere is actually a distinct node type for module imports with the simpler import x syntax.\n\nwork_w[0].body[6]\n\n<_ast.Import at 0x7ffb144b5670>\n\n\nThe unfortunate parts is that this does not have the level attribute, so we will have to manually parse the module names to determine when things were subdirectory imports:\n\nwork_w[0].body[6].names[0].name\n\n'sub.one'\n\n\n\nwork_w[0].body[7].names[0].name\n\n'sub.subsub.two'\n\n\n\n\nInline Imports\nOur function definition include two inline import statments:\ndef do_stuff():\n    import re\n    import talk as tk\n    print(f\"{greeting()}, {name!\")\n\ncool_function = work_w[0].body[3]\ncool_function\n\n<_ast.FunctionDef at 0x7ffb144b5040>\n\n\n\ncool_function.name\n\n'do_stuff'\n\n\nAs with the module node, we can see these imports in the body of the function definition node:\n\ncool_function.body\n\n[<_ast.Import at 0x7ffb144b50d0>,\n <_ast.Import at 0x7ffb144b5160>,\n <_ast.Expr at 0x7ffb144b5280>]\n\n\n\ncool_function.body[0].names[0].name\n\n're'\n\n\n\ncool_function.body[1].names[0].name\n\n'talk'\n\n\n\ncool_function.body[1].names[0].asname\n\n'tk'"
  },
  {
    "objectID": "posts/graph/Graph_My_Code_1.html#calls",
    "href": "posts/graph/Graph_My_Code_1.html#calls",
    "title": "Graph My Code 1: Creating a Graph of Function Dependencies in Python",
    "section": "Calls",
    "text": "Calls\nNext we need to better understand how to unpack the information in a Call node.\n\nTrace Uses\n\n!cat example/abyss.py\n\nimport numpy as np\n\nz = np.zeroes(5)\n\ndef mul():\n    a = np.array([[1, 0],\n                  [0, 1]])\n    b = np.array([[4, 1],\n                  [2, 2]])\n    return np.matmul(a, b)\n\ndef eigs_of_product():\n    a = np.array([[1, 0],\n                  [0, 1]])\n    b = np.array([[4, 1],\n                  [2, 2]])\n    product = np.matmul(a, b)\n    eigs = np.linalg.eigs(product)\n    np.linalg.debug.depth.error_print(eigs)  # this is a fake call, comment out if you want to run this\n    return eigs\n\n\n\nwork_w = walk_script(\"example/abyss.py\")\n\n\nfrom collections import Counter\n\nWe build a counter of node types:\n\ntype_counter = Counter()\nfor node in work_w:\n    type_counter[type(node)] += 1\n\nFrom this (comparing how many times we used np in the script to these counts), we can tell that the information about the numpy calls are found in nodes with type _ast.Call\n\ntype_counter\n\nCounter({_ast.Module: 1,\n         _ast.Import: 1,\n         _ast.Assign: 7,\n         _ast.FunctionDef: 2,\n         _ast.alias: 1,\n         _ast.Name: 23,\n         _ast.Call: 9,\n         _ast.arguments: 2,\n         _ast.Return: 2,\n         _ast.Expr: 1,\n         _ast.Store: 7,\n         _ast.Attribute: 13,\n         _ast.Constant: 17,\n         _ast.Load: 41,\n         _ast.List: 12})\n\n\nSo, we grab just the calls like this:\n\ncalls = [n for n in work_w if isinstance(n, ast.Call)]\n\n\ncalls\n\n[<_ast.Call at 0x7ffb144f1b20>,\n <_ast.Call at 0x7ffb144f1be0>,\n <_ast.Call at 0x7ffb144f1490>,\n <_ast.Call at 0x7ffb144f10a0>,\n <_ast.Call at 0x7ffb1459cfd0>,\n <_ast.Call at 0x7ffb1454fd30>,\n <_ast.Call at 0x7ffb1454f910>,\n <_ast.Call at 0x7ffb1458a760>,\n <_ast.Call at 0x7ffb1458a580>]\n\n\nThis is the basic structure of the call data:\n\ncalls[0].func.value.id\n\n'np'\n\n\n\ncalls[0].func.attr\n\n'zeroes'\n\n\n\ncalls[0].func.value.lineno\n\n3\n\n\nNested calls (using a submodule, like when we use np.linalg.eigs) require different handling\n\ncall = calls[-2]\n\n\nfunction = call.func\n\n\nfunction\n\n<_ast.Attribute at 0x7ffb1458a340>\n\n\n\nfunction.value.attr\n\n'linalg'\n\n\n\nfunction.value.value.id\n\n'np'\n\n\n\nfunction.attr\n\n'eigs'\n\n\nAs do deeply nested calls (np.linalg.debug.depth.error_print(eigs))\n\ncall = calls[-1]\n\n\nfunction = call.func\nfunction\n\n<_ast.Attribute at 0x7ffb1458a670>\n\n\n\nfunction.attr\n\n'error_print'\n\n\n\nfunction.value.attr\n\n'depth'\n\n\nyou just keep grabbing .value until the type changes from ast.Attribute to ast.Name\n\nfunction.value\n\n<_ast.Attribute at 0x7ffb1458ad60>\n\n\n\nfunction.value.value\n\n<_ast.Attribute at 0x7ffb1458a700>\n\n\n\nfunction.value.value.attr\n\n'debug'\n\n\n\nfunction.value.value.value\n\n<_ast.Attribute at 0x7ffb1458a730>\n\n\n\nfunction.value.value.value.attr\n\n'linalg'\n\n\n\nfunction.value.value.value.value\n\n<_ast.Name at 0x7ffb1458aca0>\n\n\n\nfunction.value.value.value.value.id\n\n'np'\n\n\n\nfunction.attr\n\n'error_print'\n\n\nIn some of these examples we can begin to see an issue we’ll need to resolve later. Really, rather than use the walked list of nodes in the AST, we want to recursively walk from the top node and also use recursive calls to extract data from the nodes. More on that in the next article."
  },
  {
    "objectID": "posts/graph/mermaid.html",
    "href": "posts/graph/mermaid.html",
    "title": "Intro to Mermaid",
    "section": "",
    "text": "Mermaid is a tool that lets you create markdown-type descriptions of graphs/flowcharts and converts them to diagrams.\nAs an example:\ngraph LR;\n    A--> B & C & D;\n    B--> A & E;\n    C--> A & E;\n    C--> A & E;\n    E--> B & C & D;\nbecomes this when rendered:\n\n\n\n\ngraph LR;\n    A--> B & C & D;\n    B--> A & E;\n    C--> A & E;\n    C--> A & E;\n    E--> B & C & D;\n\n\n\n\n\n\n\n\nIt’s possible to dynamically edit them with VSCode extensions or with online editors.\nThis is great for visualizing small graphs and programtically producing flow charts. Manually creating a flowchart is painful enough, not too mention having to maintain it as the inevitable changes roll in. Keeping the diagram in code reduces these headaches."
  },
  {
    "objectID": "posts/graph/mermaid.html#examples",
    "href": "posts/graph/mermaid.html#examples",
    "title": "Intro to Mermaid",
    "section": "Examples",
    "text": "Examples\nTo use these in markdown, put them in a code block with type mermaid (or type {mermaid} if using Quarto). Meaning, put the the example (stuff) like this (replacing the single quotes ' with backticks ` because making markdown examples in markdown is difficult):\n'''mermaid\nstuff\n'''\n\nMinimal\n\n\n\n\ngraph LR;\n    A;\n\n\n\n\n\n\n\n\ngraph LR;\n    A;\nYou have to say:\n\ngraph - this is a graph-type diagram\nLR - nodes are produced left to right. Without this orientation it bugs out.\nA; - the first node has the label “A”\n\n\n\nDecision Flowchart\n\n\n\n\ngraph TB;\n    Start-->|evaluate| Consider;\n    Consider--certain--> Done;\n    Consider--uncertain--> mr[More Research];\n    mr -->|re-evaluate| Consider;\n\n\n\n\n\n\n\n\ngraph TB;\n    Start-->|evaluate| Consider;\n    Consider-->|certain| Done;\n    Consider-->|uncertain| mr[More Research];\n    mr -->|re-evaluate| Consider;\n\ngraph and flowchart are interchangeable diagram types\nid[longer description] allows you to alias the contents of a node (and also handles spaces in node labels\n-->|edge label| provides a text label for an edge\n\n\n\nLarger Graph\n\n\n\n\ngraph LR;\n    a --- b;\n    a --- b;\n    b --- c;\n    a --- c;\n    c --- d;\n    d --- e;\n    e --- f;\n    c --- f;\n    g --- h;\n    g --- i;\n    g --- j;\n    g --- k;\n    i --- i1;\n    i --- i2;\n    i --- i3;\n    i --- i4;\n\n\n\n\n\n\n\n\ngraph LR;\n    a --- b;\n    a --- b;\n    b --- c;\n    a --- c;\n    c --- d;\n    d --- e;\n    e --- f;\n    c --- f;\n    g --- h;\n    g --- i;\n    g --- j;\n    g --- k;\n    i --- i1;\n    i --- i2;\n    i --- i3;\n    i --- i4;\n\nwe use --- for undirected edges and --> for directed\nthe graph can be disconnected\nyou can have multiple edges between nodes (a and b here)"
  },
  {
    "objectID": "posts/graph/mermaid.html#application-scrape-python-code-to-generate-a-dependency-graph",
    "href": "posts/graph/mermaid.html#application-scrape-python-code-to-generate-a-dependency-graph",
    "title": "Intro to Mermaid",
    "section": "Application: Scrape Python Code to Generate a Dependency Graph",
    "text": "Application: Scrape Python Code to Generate a Dependency Graph\nLet’s say you have some code where you want to show which modules or calling which other modules. This is nicely described using a dependency graph. For example:\n\n\n\n\ngraph LR;\n        main --> worker;\n        helper_2 --> numpy;\n        worker --> helper_1;\n        worker --> helper_2;\n\n\n\n\n\n\n\n\nThis tells us the main function calls the worker, the worker calls two helpers, and one of the helpers calls numpy. Generating this by hand would be a nuisance! Not only to just go through all of the files and read the imports then open up your least favorite flow chart tool (powerpoint). Then, what happens if the code changes (which happens more than you think)?\nLuckily, it’s easy to create a parser and a script to generate the Mermaid description of the code.\nTo show how, let’s double-down and demonstrate a quick parser by first viewing the parser dependencies using the parser itself:\n\n\n\n\ngraph LR;\n        py_code_scraper[py_code_scraper] --> os[os];\n        py_code_scraper[py_code_scraper] --> import_finder[import_finder];\n        import_finder[import_finder] --> re[re];\n        create_python_code_g[create_python_code_graph] --> pathlib[pathlib];\n        create_python_code_g[create_python_code_graph] --> py_code_scraper[py_code_scraper];\n        create_python_code_g[create_python_code_graph] --> generate_mermaid_des[generate_mermaid_desc];\n\n\n\n\n\n\n\n\nThen we have these four snippets comprising a simple parser and Mermaid dependency graph description:\n\ncreate_python_code_graph.py\n\nfrom pathlib import Path\nfrom py_code_scraper import scrape_module_graph\nfrom generate_mermaid_desc import generate_desc\n\ncode_directory = Path(\".\")\nmodule_graph_edges = scrape_module_graph(code_directory)\nmermaid_desc = generate_desc(module_graph_edges)\n\nprint(mermaid_desc)\n\npy_code_scraper.py\n\nimport os\nfrom import_finder import find_imports\n\n\ndef scrape_module_graph(dir_name):\n    module_graph = []\n    for file in os.listdir(dir_name):\n        if file.endswith(\"py\"):\n            code_file = dir_name / file\n            module_name = file.split(\".\")[0]\n            with open(code_file, \"r\") as f:\n                imported_modules = find_imports(f.readlines())\n                edges = [(module_name, import_module) for import_module in imported_modules]\n                if edges:\n                    module_graph.extend(edges)\n    return module_graph\n\ngenerate_mermaid_desc.py\n\ndef update_module_name_lookup(module_name, module_lookup_dict):\n    if len(module_name) > 20:\n        module_lookup_dict[module_name] = module_name[:20]\n    else:\n        module_lookup_dict[module_name] = module_name\n\ndef generate_desc(import_graph_edges):\n    contents = []\n    header = \"```{mermaid}\"\n    figure_type = \"graph LR;\"\n    footer = \"```\"\n    contents.append(header)\n    contents.append(figure_type)\n    module_lookup = {}\n    for (s, t) in import_graph_edges:\n        s_name = module_lookup.get(s, \"\")\n        t_name = module_lookup.get(t, \"\")\n        if not s_name:\n            update_module_name_lookup(s, module_lookup)\n            s_name = module_lookup[s]\n        if not t_name:\n            update_module_name_lookup(t, module_lookup)\n            t_name = module_lookup[t]\n\n        edge_line = f\"\\t{s_name}[{s}] --> {t_name}[{t}];\"\n        contents.append(edge_line)\n    contents.append(footer)\n    return \"\\n\".join(contents)\n\nimport_finder.py\n\nimport re\n\n\ndef find_imports(code):\n    import_list = []\n    for line in code:\n        if line.startswith(\"import\"):\n            import_parts = line.split(\" \")\n            imported_module = import_parts[1].strip()\n            import_list.append(imported_module)\n        if line.startswith(\"from\"):\n            import_parts = line.split(\" \")\n            imported_module = import_parts[1].strip()\n            import_list.append(imported_module)\n    return import_list\nSo, when running the dependency graph creator on itself we get:\ngraph LR;\n        py_code_scraper[py_code_scraper] --> os[os];\n        py_code_scraper[py_code_scraper] --> import_finder[import_finder];\n        import_finder[import_finder] --> re[re];\n        create_python_code_g[create_python_code_graph] --> pathlib[pathlib];\n        create_python_code_g[create_python_code_graph] --> py_code_scraper[py_code_scraper];\n        create_python_code_g[create_python_code_graph] --> generate_mermaid_des[generate_mermaid_desc];\nFairly straight-forward, if not full-featured (for instance, I’m not handling folder crawling here or providing more detail on the dependencies). But this shows you how you can quickly hack together an automated graph descriptions.\nIf you are interested in this particular use case, there is a similar tool to create a dependency graph figure with Python (though it doesn’t use Mermaid from what I can see) called pydeps and I’m sure if you search more you can find one that does created the Mermaid description."
  },
  {
    "objectID": "posts/graph/mermaid.html#references",
    "href": "posts/graph/mermaid.html#references",
    "title": "Intro to Mermaid",
    "section": "References",
    "text": "References\n\nMermaid docs\nGraph Syntax"
  },
  {
    "objectID": "posts/old/2020-08-02-Python_and_MIDI_2.html",
    "href": "posts/old/2020-08-02-Python_and_MIDI_2.html",
    "title": "Make More Interesting Random Music",
    "section": "",
    "text": "We saw last time that it is simple to construct a MIDI file with the pretty_midi package. Now to make something a little more musically interesting than alternating between two notes we need to randomly generate notes to play using some basic music theory to make things sound “good.” To do this we will:\n\nchoose a scale\nfind all of the midi notes attached to that scale\nrandomly draw notes from that collection of midi notes\nget a pleasing collection of simple chords to accompany the melody\nvary the rhythm of the melody\n\n\n\nFirst we use a couple of nice utilities of pretty_midi, a method which takes an integer and spits out one of the major and minor keys and another method which takes that same integer and returns how many accidentals the key has. Using the key name we can determine whether the key has flat accidentals or sharp accidentals and then by using the fact that “accidentals accumulate” (if a key has a G# then it also has a C#, for example) we can easily identify which notes are in the key.\nTo say more, we are using the fact that the Major and Minor keys have certain nice patterns. If we start from C and move up in perfect 5ths (C, G, D, A) then those corresponding (major) keys each add an accidental:\n\nC Major: CDEFGAB\nG Major: GABCDEF#\nD Major: DEF#GABC#\nA Major: ABC#DEF#G#\n…\n\nThis means we know the exact notes that are made sharp or (flat) if we know if the key is sharp (flat) and how many accidentals there are, we do not even need to know the root: we are being told the same information in a different way. Now, there is actually a more insightful way to do this (how making major scales is normally taught) by using the fact that you start at the root and add notes with the pattern WWHWWWH, but it was fun to think of a different way to get the notes.\nWe return the key_notes which runs from C to B because this is how the MIDI format is laid out for each octave (… B1 C2 C#2 … A#2 B2 C3 …), but we also save off the notes of the key starting from the root for (optional) printing to the user as scale_notes.\nnote_names = [n for n in 'CDEFGAB']\nsharp_accidentals = [n for n in 'FCGDAEB']\nflat_accidentals = [n for n in 'BEADGCF']\n\ndef determine_key_notes(key_number):\n    key_name = pretty_midi.key_number_to_key_name(key_number)\n    _, num_accidentals = pretty_midi.key_number_to_mode_accidentals(key_number)\n    root = key_name[0]\n    root_index = note_names.index(root)\n\n    if key_name[1] == \"b\":\n        accidental_mark = 'b'\n        accidentals = flat_accidentals[:num_accidentals]\n    else:\n        accidental_mark = '#'\n        accidentals = sharp_accidentals[:num_accidentals]\n\n    key_notes = list(map(lambda n: n + accidental_mark if n in accidentals else n, note_names))\n\n    scale_notes = (key_notes + key_notes)[root_index:root_index + 7]\n\n\n\nFor this, use the pretty_midi utility that converts note names to MIDI note numbers and apply it to all the note names with all the octave numbers attached.\ndef get_all_midi_numbers(note_names):\n    midi_numbers = []\n    note_names = list(set(note_names))  # simple de-dup\n    for octave in range(-1, 9):\n        for note in note_names:\n            midi_number = pretty_midi.note_name_to_number(note + str(octave))\n            midi_numbers.append(midi_number)\n    return sorted(midi_numbers)\nWe do not need to de-duplicate or sort for our current use, but I added those steps in case I supply some note names “out of order” ([D, C] instead of [C, D] for instance) or provide possible note duplicates for other uses.\n\n\n\nHere you can now use the collection of midi notes and just make a random choice from it at each step. For instance if g_maj_midi is the collection of MIDI notes for G Major then you can randomly select one with pitch = random.choice(g_maj_midi[21:36]) where we take a slice of the array to restrict the notes to just a couple of octaves.\n\n\n\nWe want to get just simple chords from the major scale for the piano to play them for whole notes. There are some existing Python packages that can be used:\n\nchords2midi let’s you generate a MIDI file by supplying a progressing an a key: c2m I V vi IV --key C\nchords2midi uses pychord, which will generate the component notes of a chord from its name as well as name a chord from its notes.\n\nThese are both strong utilities that I will certainly use as I expand but for now I will do something much simpler. You can get a major scale chord progression by simply going to the scale and taking a note for the root and getting the third and fifth of a triad by just taking the second and fourth notes after your root. For instance, you can get a C (major) triad from the C major scale by looking at the scale CDEFGAB and using that pattern C_E_G__. Whether this is major or minor is irrelevant for our use: we just want to grab all the simple triads (not worrying about inversions or anything) and collect them together for one octave of root notes:\ndef get_major_progression(root_index, midi_numbers):\n    chords = []\n    for i in range(8):\n        chord_root_index = root_index + i\n        chord_third_index = chord_root_index + 2\n        chord_fifth_index = chord_root_index + 4\n        root = midi_numbers[chord_root_index]\n        third = midi_numbers[chord_third_index]\n        fifth = midi_numbers[chord_fifth_index]\n        chord = [root, third, fifth]\n        chords.append(chord)\n    return chords\nThis will generate a list of chord lists, where each chord list is the midi notes for a given triad. Note, if you do not feed the notes of a key as the MIDI numbers you will not get a major key progression, so this might be imperfectly named.\n\n\n\nWhen we were adding notes one at a time we were keeping track of when the note began and when it ended in seconds. It would be nice to forget about when they begin and imagine writing the song and moving forward, adding notes “now.” To do this we make a writer for our instrument that keeps track of when “now” is and allows us to add notes one at a time, as a chord, or in a chunk of notes. Using this chunking allows us to vary the rhythm, we can randomly determine how long the next note(s) should be and then play a bunch of notes of that length. This is slightly more natural than varying each note independently, because shorter notes are often grouped together.\nclass Writer:\n    def __init__(self, instrument):\n        self.position = 0\n        self.instrument = instrument\n\n    def add_note(self, pitch, length, move_forward=True):\n        note = pretty_midi.Note(velocity=100, pitch=pitch, start=self.position, end=self.position + length)\n        self.instrument.notes.append(note)\n\n        if move_forward:\n            self.position += length\n\n    def add_note_series(self, pitches, length):\n        for pitch in pitches:\n            self.add_note(pitch, length)\n\n    def add_chord(self, pitches, length, move_forward=True):\n        for pitch in pitches:\n            self.add_note(pitch, length, move_forward=False)\n        if move_forward:\n            self.position += length\nWe leave move_forward as an optional argument, because if we are writing notes that will occur at the same time (to form a chord) then we want the Writer “head” or position to stay at the same spot until we are done adding notes to that point in time. There are other simplifications that are made (no velocity changes), but this simple class gives us a lot of power so that we can more expressively generate midi music.\nimport pretty_midi\nfrom music_info import determine_key_notes\nimport music_info\nimport random\n\n# Create a PrettyMIDI object\nensemble = pretty_midi.PrettyMIDI()\n\n# Create an Instrument instance for a cello instrument\n# Changed to a guitar for my song, and was lazy about changing variable names\n# cello_program = pretty_midi.instrument_name_to_program('Cello')\ncello_program = pretty_midi.instrument_name_to_program('Overdriven Guitar')\ncello = pretty_midi.Instrument(program=cello_program)\n\n# do the same for a piano\npiano_program = pretty_midi.instrument_name_to_program('Acoustic Grand Piano')\npiano = pretty_midi.Instrument(program=piano_program)\n\n# Add the instruments to the PrettyMIDI object\nensemble.instruments.append(cello)\nensemble.instruments.append(piano)\n\n# here is where I put the Writer class from above\n\npiano_writer = Writer(piano)\ncello_writer = Writer(cello)\n\nsong_length_in_seconds = 30\nbpm = 120\nbeat_length = 60 / bpm\nnum_beats = int(song_length_in_seconds / beat_length)\n\n# Decided on G major, which is index 7\ng_maj = music_info.determine_key_notes(7)\ng_maj_midi = music_info.get_all_midi_numbers(g_maj)\nroot_number = pretty_midi.note_name_to_number(\"G4\")\nroot_index = g_maj_midi.index(root_number)\nmajor_progression = music_info.get_major_progression(root_index, g_maj_midi)\n\naccompaniment_writer = piano_writer\nsolo_writer = cello_writer\n\n# now we see the power of the writer object\nwhile accompaniment_writer.position < song_length_in_seconds:\n    length = beat_length * 4  # whole note chords\n\n    # choose a random chord from the progression\n    chord = random.choice(major_progression)  \n\n    # add a lower octave of the notes for fullness\n    larger_chord = chord + [n - 12 for n in chord]\n\n    # write the chord\n    accompaniment_writer.add_chord(larger_chord, length)\n\nwhile solo_writer.position < song_length_in_seconds:\n    # choose to play 16th, 8th, quarter, half, or whole note(s)\n    division = random.choice([-2, -1, 0, 1, 2])\n    length = beat_length * (2 ** division)\n\n    # if we chose 16th or 8th, play multiple of them\n    if division < 0:\n        num_notes = 2 ** (-1 * division)\n        pitches = random.choices(g_maj_midi[21:36], k=num_notes)\n        solo_writer.add_note_series(pitches, length)\n    else:\n        pitch = random.choice(g_maj_midi[21:36])\n        solo_writer.add_note(pitch, length=beat_length)\n\ndef write_song():\n    # Write out the MIDI data\n    ensemble.write('duo.mid')\nNow with this I created the following simple duet. It isn’t the most thrilling piece of music, but with relatively simple rules behind it, I think it is interesting how “complex” it sounds."
  },
  {
    "objectID": "posts/old/2020-07-21-fastbook_ch5.html",
    "href": "posts/old/2020-07-21-fastbook_ch5.html",
    "title": "FastBook Chapter 5 Thoughts",
    "section": "",
    "text": "After reading the first half of chapter 3 (will read the second half this week) and (mostly) breezing through Chapter 4 (it contained a lot of familiar material), I worked on Chapter 5 this weekend.\n\n\n\nPresizing.\nChecking your DataBlock before you begin training\nTrain early (get a reasonable MVP) and often (if it’s not too expensive).\nCross-Entropy Loss for the binary case and extending it to multi-class examples.\nConfusion matrix with ClassificationInterpretation and looking at the most_confused examples.\nLearning Rate Finder\nMore particulars on transfer learning, including how to use discriminative learning rates to not lose the solid training of the transferred modeled.\n\n\n\n\n\nPresizing is a particular way to do image augmentation that is designed to minimize data destruction while maintaining good performance. The general idea is to apply a composition of the augmentation operations all at once rather than iteratively augment and interpolate. This has savings both in terms of computation and the final quality of the examples.\n\nIn the 3s and 7s table there is a column labeled “loss”, which for me was a bit confusing. In the first row loss was the predicted output of the “3” class, which happened to be the correct answer. However, loss was just the output of that example, which does not quite make sense because you are looking to minimize loss which conflicts with the goal of maximizing the predicted output for the true class. It looks like this was just an oversight with the naming convention because to compute the loss more things are done and the text that follows makes this clear.\nI found it useful to explicitly calculate the loss in the binary example provided.\nActivations:\nacts[0, :]\n\ntensor([0.6734, 0.2576])\n\nclass0_act = acts[0, 0]\nclass1_act = acts[0, 1]\nclass0_act\n\ntensor(0.6734)\n\nComputing the exponential of the activations to then get the softmax.\nfrom math import exp\nexp0 = exp(class0_act)\nexp0\n\n1.9608552547588787\n\nexp1 = exp(class1_act)\nsmax0 = exp0 / (exp0 + exp1)\nsmax0\n\n0.602468670578454\n\nsmax1 = exp1 / (exp0 + exp1)\nsmax1\n\n0.39753132942154595\n\nsmax0 + smax1\n\n1.0\n\nfrom math import log\nlog(smax0)\n\n-0.5067196140092344\n\nlog(smax1)\n\n-0.9224815318387478\n\n-log(smax0)\n\n0.5067196140092344\n\nAnd that is the loss for the first example, because the true class was 0. This matches the calculation using the fastai classes, which is always a relief.\n\n\n\n\nCyclical Learning Rates for Training Neural Networks\nHow transferable are features in deep neural networks?\n\n\n\n\n\nWhy do we first resize to a large size on the CPU, and then to a smaller size on the GPU?\nYou want to create a uniform input size for your data and also apply various transformations to augment it. The presizing method, running augmentation transformations as a single composition rather than iteratively, allows you to have larger/more “rich” inputs to transform before making them a smaller, uniform size that you will train the model with.\nWhat are the two ways in which data is most commonly provided, for most deep learning datasets?\n\nA collection of data elements, that have filenames indicating information about them, like their class. (A folder of pictures where each picture has a file name with its ID and class).\nTabular format that can either contain the data in each row (along with the metadata) or point to data in other formats. (A csv file with ID, true class, and a hyper link to the input picture.)\n\nLook up the documentation for L and try using a few of the new methods is that it adds.\nL is a beefier list class. How it’s different from a regular list:\n\nthe print function is smarter. It provides the length of the list and truncates the end, which is nice if you’ve ever crashed a server because you accidentally printed out an obscenely long list.\nyou can access L with a tuple, whereas a normal list will break if you try to access it that way.\nit has unique(), which functions like the same method in Pandas.\nit has a filter method attribute.\n\nLook up the documentation for the Python pathlib module and try using a few methods of the Path class.\nPath was introduced to Python in 3.4. It appears to combine a bunch of common things that you typically use os with along with the ability to manage file paths without doing string manipulations (as well as reducing the \\ vs / mistakes that are frequently made).\nOne nice thing that can be done, set here = Path('.') and then iterate over the current directory with for f in here.iterdir(): print(f). You can also .open() a path object rather than feeding it to open() and do glob stuff.\nGive two examples of ways that image transformations can degrade the quality of the data.\n\nImage simply rotating a square 45 degrees to stand it up on one corner. Now the new image that you get (take an old square position cut out of the rotated square position) is missing anything in the corners, so it has to be interpolated. This loses about 17% of the original image, so it’s pretty significant!\nBrightening an image will move the brighter pixels up to the maximum brightness, so their original brightness cannot be recovered by simply redarkening.\n\nWhat method does fastai provide to view the data in a DataLoaders?\nYou can use .show_batch(nrows, ncols) on the DataLoaders object to get a grid of some of the examples.\nWhat method does fastai provide to help you debug a DataBlock?\nUsing .summary() on the DataBlock object gives a verbose attempt to try and create the batch. The output from this, along with errors that come up if it fails can help you notice a problem.\nShould you hold off on training a model until you have thoroughly cleaned your data?\nNo, sometimes life is easy! Also, it’s good to get reasonable bench marks as soon as possible. Not only to help game-ify the problem and motivate you to work on it, but also to have a baseline to see if the room for improvement is worth the energy.\nWhat are the two pieces that are combined into cross-entropy loss in PyTorch?\nnn.CrossEntropyLoss (see the docs) applies nll_loss after log_softmax (which is log of softmax)\nWhat are the two properties of activations that softmax ensures? Why is this important?\n\nYou can interpret activations as probabilities.\n\noutputs sum to one\noutputs are non-negative\n\nIt forces the model to favor a single class.\n\nThis more relevant behavior that is mentioned that it amplifies small differences, which is useful if you want the network to be somewhat decisive rather than having all outputs close to each other.\nWhen might you want your activations to not have these two properties?\nThe parenthetical comment in the main text mentions that you may not want the model to pick a class just because it has a slightly larger output. You want the model to be sure about the class, not just relatively sure.\nFor the probability property, it might be misleading because it isn’t necessarily the actual probability of the example being that class.\nWhy can’t we use torch.where to create a loss function for datasets where our label can have more than two categories?\nIn part, this is a constraint of the where function. Where selects between two outputs based on a condition. It is too difficult to right a nested condition when you have more than two outcomes and selecting the loss requires a bit more work, so this trick becomes way less convenient.\nWhat are two good rules of thumb for picking a learning rate from the learning rate finder?\n\n\nOne order of magnitude less than where the minimum loss was achieved (i.e., the minimum divided by 10).\nThe last point where the loss was clearly decreasing\n\n\nWhat two steps does the fine_tune method do?\nWe go to the source:\nself.freeze()\nself.fit_one_cycle(freeze_epochs, slice(base_lr), pct_start=0.99, **kwargs)\nbase_lr /= 2\nself.unfreeze()\nself.fit_one_cycle(epochs, slice(base_lr/lr_mult, base_lr), pct_start=pct_start, div=div, **kwargs)\nThus it: 1. Performs a one-cycle fit with the pre-trained layers frozen (their weights do not update). 2. Performs another one-cycle fit with the pre-trained layers unfrozen at half the learning rate.\nWhat are discriminative learning rates?\nThe idea here is that you may want weights in certain layers to change at a different rate. In particular, if your first layers come from a pretrained network you may want to do updates to them more slowly than the last layers which are tailored to your particular problem.\nHow is a Python slice object interpreted when passed as a learning rate to fastai?\nIt acts like numpy.linspace where the num is implicitly defined as the number of layers.\nWhy is early stopping a poor choice when using 1cycle training?\nFor a description of 1cycle training, the fastai docs refer to the rate finder paper in the references as well as this blog post. It looks like the basic idea is stopping early does give the training a chance to be finely tuned, because you are likely stopping at a point where the learning rate is still large.\nWhat is the difference between resnet50 and resnet101?\nBoth resnet50 and resnet100 are residual networks, and seem to have been introduced in Deep Residual Learning for Image Recognition. The basic idea of deep residual networks seems to be “wire a network that is trying to learn the function \\(\\mathcal{H}(x)\\) such that it has to learn \\(\\mathcal{H}(x) - x\\) instead.” The intuition being that, for example, it is easier to learn the zero function than it is to learn the identity function. resnet-50 looks like it was obtain from the resnet-34 architecture by replacing certain layer pairs with layer triplets known as bottleneck blocks. resnet-101 (and resnet-152) are just an expansion of this idea, adding 17 more (or 34 more) of these triplet-layer blocks.\nWhat does to_fp16 do?\n\nThe other downside of deeper architectures is that they take quite a bit longer to train. One technique that can speed things up a lot is mixed-precision training. This refers to using less-precise numbers (half-precision floating point, also called fp16) where possible during training. As we are writing these words in early 2020, nearly all current NVIDIA GPUs support a special feature called tensor cores that can dramatically speed up neural network training, by 2-3x. They also require a lot less GPU memory. To enable this feature in fastai, just add to_fp16() after your Learner creation (you also need to import the module)."
  },
  {
    "objectID": "posts/old/2020-08-01-Python_and_MIDI.html",
    "href": "posts/old/2020-08-01-Python_and_MIDI.html",
    "title": "Making Music with pretty_midi",
    "section": "",
    "text": "A while back, when I was first learning Python, a friend and I made a program that generated random music by iterating through a song one note at a time randomly picking the note length and pitch for the notes with increasingly strict rules. My friend figured out how to use MIDIUtil to create a midi file by writing down a sequence of note events, where each event says something about:\n\nvelocity - how loud the note should be played\npitch - an integer value for telling a midi play what frequency to play\nnote start - when the note should start playing\nnote end - when it should stop\n\nYou can do a lot of music with just that information, and if you make simple choices for notes, with the right MIDI player you can get surprisingly listenable music. I have wanted to expand on this program for a while in various ways:\n\ngenerating music by applying machine learning techniques to various collections of midi files (like this Google Bach doodle)\nmaking something more interactive (something involving the console where you can add motifs and ideas on the play and have them played back)\nfeed the program just a text file with a simplistic music notation to easily create ideas that can get more complex (like TidalCycles which uses Haskell and SuperCollider).\n\n\n\n\nI searched around on pypi a bit and found a promising package to start writing MIDI files with: pretty_midi. It’s a project on GitHub that seems to still be active (part of why I am not just using MIDIUtil again is that I remember I slightly annoying setup and it seems to be inactive). After a simple install, and running the second example from the documentation I found that it easily generated pleasing sounds that (after a bit of fitzing) I could simply listen to with VLC Media Player. Before I just imported the files into Reaper where I could use a custom VST, but this makes iterating a bit quicker.\nHere is an example I made where two instruments are playing a very minimalist piece:\nimport pretty_midi\n\n# Create a PrettyMIDI object\nensemble = pretty_midi.PrettyMIDI()\n\n# Create an Instrument instance for a cello instrument\ncello_program = pretty_midi.instrument_name_to_program('Cello')\ncello = pretty_midi.Instrument(program=cello_program)\n\n# do the same for a piano\npiano_program = pretty_midi.instrument_name_to_program('Acoustic Grand Piano')\npiano = pretty_midi.Instrument(program=piano_program)\n\n# Add the instruments to the PrettyMIDI object\nensemble.instruments.append(cello)\nensemble.instruments.append(piano)\n\n\nsong_length_in_seconds = 30\nbpm = 120\nbeat_length = 60 / bpm\nnum_beats = int(song_length_in_seconds / beat_length)\n\nfor beat in range(num_beats):\n    if beat % 2 == 0:\n        note_name = 'C5'\n    else:\n        note_name = 'D5'\n    note_number = pretty_midi.note_name_to_number(note_name)\n    note_start = beat * beat_length\n    note_end = note_start + beat_length\n    note = pretty_midi.Note(velocity=100, pitch=note_number, start=note_start, end=note_end)\n    cello.notes.append(note)\n    piano.notes.append(note)\n\n\ndef write_song():\n    # Write out the MIDI data\n    ensemble.write('ensemble.mid')\n\nif __name__ == \"__main__\":\n    write_song()\nThis is a simple example, yet it shows the basics of the process that I will expand on:\n\nadd notes one at a time\nincorporate some randomness into the note properties (here pitch changes deterministically, but that will soon change)\nusing multiple instruments\n\nI will begin to add to this, hopefully reaching what we achieved during my first experience with Python and MIDI and then think about ways to expand it and then start to apply some of my fastbook reading to it."
  },
  {
    "objectID": "posts/old/2020-09-30-reading_in_a_wave_file.html",
    "href": "posts/old/2020-09-30-reading_in_a_wave_file.html",
    "title": "Reading in a Wave File",
    "section": "",
    "text": "The sound I created was simple 2 seconds of a sine wave at around C4. Zooming in to a 0.045 second window we can see the wave in Reaper. A simple peak-to-peak measure was 0.00375 seconds long which gives a frequency around 267 herz. Since this was a C4, which is typically 262 it is not too far off.\nTo read the file a quick search brought up the wavfile method from scipy. I basically follow the example provided there on my sound.\n\nfrom scipy.io import wavfile\nfrom pathlib import Path\n\n\np = Path('sounds')\n\n\nq = p / 'simple_c4.wav'\n\n\nq\n\nWindowsPath('sounds/simple_c4.wav')\n\n\n\nsample_rate, data = wavfile.read(q)\n\nc:\\python37\\lib\\site-packages\\scipy\\io\\wavfile.py:273: WavFileWarning: Chunk (non-data) not understood, skipping it.\n  WavFileWarning)\n\n\n\nsample_rate\n\n44100\n\n\n\ndata\n\narray([[ 0.0000000e+00,  0.0000000e+00],\n       [ 0.0000000e+00,  0.0000000e+00],\n       [-3.8028106e-06, -3.8028106e-06],\n       ...,\n       [-6.6743125e-03, -6.6743125e-03],\n       [-4.4752425e-03, -4.4752425e-03],\n       [-2.2473824e-03, -2.2473824e-03]], dtype=float32)\n\n\n\ndata.shape\n\n(88200, 2)\n\n\nSince the recording is 2 seconds long and the sample rate is 44100, there are 88200 samples total, as expected. This is a list of channel lists. So, the stereo data is recorded for each sample.\n\nlength = data.shape[0] / sample_rate\nlength\n\n2.0\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ntime = np.linspace(0., length, data.shape[0])\n\nIn this case the channels are the same, so just plotting the first one:\n\nplt.plot(time, data[:, 0])\nplt.xlabel(\"Time [s]\")\nplt.ylabel(\"Amplitude\")\nplt.show()\n\n\n\n\nToo much info for this tiny graph! Let’s look at just one second and then zoom in more.\n\nspan_length = data.shape[0] // 2\n\n\nspan_length\n\n44100\n\n\n\nplt.plot(time[:span_length], data[:span_length, 0])\nplt.xlabel(\"Time [s]\")\nplt.ylabel(\"Amplitude\")\nplt.show()\n\n\n\n\n\nspan_length = data.shape[0] // 20\n\n\nspan_length\n\n4410\n\n\n\nplt.plot(time[:span_length], data[:span_length, 0])\nplt.xlabel(\"Time [s]\")\nplt.ylabel(\"Amplitude\")\nplt.show()\n\n\n\n\n\nspan_length = data.shape[0] // 200\n\n\nspan_length\n\n441\n\n\n\nstart_sample = 500\n\n\nplt.plot(time[start_sample:start_sample+span_length], data[start_sample:start_sample+span_length, 0])\nplt.xlabel(\"Time [s]\")\nplt.ylabel(\"Amplitude\")\nplt.show()\n\n\n\n\nA simple sine wave!"
  },
  {
    "objectID": "posts/old/2020-07-23-data_objects.html",
    "href": "posts/old/2020-07-23-data_objects.html",
    "title": "DataPlox",
    "section": "",
    "text": "DataBlock\nThis is a fastai object which helps you build Datasets and DataLoaders. In addition to Chapter 6 of the fastbook, there is also a tutorial in the fastai docs. A DataBlock is a blueprint for how to build your data. You tell it:\n\nwhat kind of data you have (blocks=),\nhow to get the input data (get_x= or get_items=),\nhow to get the targets/labels (get_y),\nhow to perform the train/validation split (splitter=),\nas well as any resizing (items_tfms=) or augmentations you want to be performed (batch_tfms).\n\nBy feeding these blue prints a source (like a directory on your computer), you can use a DataBlock to create a Datasets or DataLoaders object.\n\n\nDataset\nDataset is a Torch object. We can find out exactly what a Dataset is because PyTorch is open source. We just have to be brave enough to parse some of the grittier implementation details.\nAccording to the source code a Dataset is at its core something that allows you to grab an item if you provide the index/key for it and that you can also add items to. This is just the abstract class definition, essentially the bare bones of a what a dataset should be. If you try to make a class that inherits from Dataset you will get an error if you do not implement __getitem__, the method for grabbing items. It does this by setting the default behavior of that method to raise a NotImplementedError. You can also implement this behavior (forcing inheriting classes to define specific methods) by using the abc package. The source code also mentions that it would have set a default for a length function, but the standard methods for making a default that is forced to change have conflicts with what a length function is “supposed” to do.\nThe types of Datasets are:\n\nIterableDataset\nTensorDataset\nConcatDataset\nChainDataset\nSubset\n\n\n\nDatasets\nDatasets is an object that contains a training Dataset and a validation Dataset. You can generally construct a Datasets object from a DataBlock like this:\ndblock = DataBlock(blue_print_details)\ndsets = dblock.datasets(source)\n\n\nDataLoader\nA DataLoader is a Dataset together with a Sampler. A Sampler is a way to create an iterator out of your Dataset, so you can do things like consume data in batches as needed. Rather than take a Dataset an manually loop through chunks of it, at each step using a chunk to update a model, a DataLoader bundles this idea together. This makes a lot of sense to encapsulate: going through your data in batches is a frequently encountered process in machine learning!\nWe can see from the source code that a Sampler is at minimum:\n\na way to iterate over indices of dataset elements (__iter__) and\na way to calculate the size of the returned iterators (__len__).\n\nJust like with DataSet, defining the length method is not strictly enforced by the interpreter because the various NotImplemented errors you can throw do not quite work.`\nThe kinds of samplers:\n\nSequentialSampler - go in direct 0, 1, 2, … order.\nRandomSampler - randomly choose observations, with or without replacement (replacement=)\nSubsetRandomSampler - randomly sample from a provided subset of indices, without replacement\nWeightedRandomSampler - for non-uniform random sampling\nBatchSampler - generate mini-batches of indices\n\nFor DataLoader, the definition is a bit more involved. In part, because it implements multiprocessing, but it also does things like creating a Sampler from the arguments if one wasn’t provided.\n\n\nDataLoaders\nDataLoaders is an object that contains a training DataLoader and a validation DataLoader. You can construct a DataLoaders from a DataBlock similarly to the Datasets method:\ndblock = DataBlock(blue_print_details)\ndls = dblock.dataloaders(source)"
  },
  {
    "objectID": "posts/old/2020-07-11-fastai_book_ch2.html",
    "href": "posts/old/2020-07-11-fastai_book_ch2.html",
    "title": "FastAI Book Chapter 2",
    "section": "",
    "text": "I went through the second chapter of the book today, which is why this blog even exists. Highly useful; looking forward to working on some music projects to really learn the material.\n\n\n\nHow to easily build an image classifier to discern between blue jays, mockingbirds, and shrikes.\n\nI chose the first two bird types based on my familiarity and their Cool Local Bird ranks. I was going to select a cardinal as a third class, but thought it would be too easy to detect the class based solely on color, so I searched around and found shrikes, which looked similar to mockingbirds. The classifier performed very well, which was I semi-shock because I didn’t know:\n\nYou do not always need a lot of data to build a decent model.\n\nEven with 150 examples of each of three classes (before train/validation split), there were only 3 misclassifications on the validation set. This shows how powerful data augmentation can be, but also helps dispel the notion that you need tons of data to do anything reasonable.\n\nFastAI has some powerful tools that I need to learn.\n\n\nEasily display the examples with the highest loss and lowest confidence to see if I can interpret possible deficiencies with interp.plot_top_losses().\nClean up the dataset with ImageClassifierCleaner, manually going through some of the examples to change labels or remove them, and then effect the changes with a couple simple for-loops.\nUse verify_images to clean up corrupted files easily.\nUse DataBlock to define the structure of the problem and implement useful transformations for the data.\n\n\nThink about how to normalize images.\n\nSquashing a large image into a smaller one with simple scaling or adding cropped parts of small images is maybe not the best way to make sure the inputs are all “from the same distribution”. You can use RandomResizedCrop to maintain original image quality and also artificially expand the size of your training set.\n\nOther things:\n\nHow to easily build a dataset with the Bing Image API.\nThe usefulness of Path (which I only recently learned about) was really demonstrated nicely.\nThe Drivetrain Approach.\nCreating a Notebook App with widgets and Voilà. (Admittedly, I’m still trying to get the latter to work.)\n\n\n\n\n\n\nHow do you find the Bing Image Search key?\n\nSolution: I signed up for the free Azure thing, went to to the Bing Image Search API, and just had to click to add Bing Search APIs v7 to my subscription. Once I did this it brought up a page with the keys and endpoints.\n\nI am using my local computer, with Windows, to run the notebooks. So, I have run into (standard) issues. Namely, I saw RuntimeError: cuda runtime error (801) : operation not supported at... when I first attempted to fine tune the learner learn.fine_tune(4).\n\nSolution: When you google the error this issue page points you to the forum, but also usefully mentions the “need to set num_workers=0 when creating a DataLoaders because Pytorch multiprocessing does not work on Windows.” So, doing this when you define the dataloaders dls = bears.dataloaders(path, num_workers=0) cleared it up for me.\n\nSome minor formatting issues with interp.plot_top_losses(5, nrows=1). The text above the images was overlapping, because my class names were a bit long.\n\nSimple fix was to set nrows=5."
  },
  {
    "objectID": "posts/old/2020-08-15-huggingface_test.html",
    "href": "posts/old/2020-08-15-huggingface_test.html",
    "title": "Trying Out HuggingFace",
    "section": "",
    "text": "After going through the pain of converting a Notebook to a markdown file and then editing that markdown file to look nice (in my last post), I saw that there was a better way to hand that process: fastpages. The process was slightly rocky, but I finally think I have things more or less figured out, including linking it to a domain under my name!\nAs a first test of the capability of uploading a notebook to a blog post, I am going to toy with the Hugging Face models. Interesting name for a company/group, with lots of Alien vibes. I saw this super cool tweet: > twitter: https://twitter.com/huggingface/status/1293240692924452864?s=20\nPer the instructions here I made a virtual environment to try out some transformers:\npyenv virtualenv 3.8 hface\npyenv activate hface\npip install jupyter\npip install --upgrade pip\npip install torch\npip install transformers\nAnd then I tested with:\npython -c \"from transformers import pipeline; print(pipeline('sentiment-analysis')('I hate you'))\"\nWhich gave a correct sentiment score, I think at least, a negative score close to 1.\n\nfrom transformers import pipeline\n\n\npipeline('sentiment-analysis')('jog off')\n\n[{'label': 'NEGATIVE', 'score': 0.905813992023468}]\n\n\n\npipeline('sentiment-analysis')('exactly')\n\n[{'label': 'POSITIVE', 'score': 0.9990326166152954}]\n\n\n\npipeline('sentiment-analysis')('I saw this super cool tweet')\n\n[{'label': 'POSITIVE', 'score': 0.998775064945221}]\n\n\nVery cool!"
  },
  {
    "objectID": "posts/old/2020-08-15-huggingface_test.html#trying-out-pipelines",
    "href": "posts/old/2020-08-15-huggingface_test.html#trying-out-pipelines",
    "title": "Trying Out HuggingFace",
    "section": "Trying Out Pipelines",
    "text": "Trying Out Pipelines\nI attempted running a zero-shot classifier, but got an error (\"Unknown task zero-shot-classification, available tasks are ['feature-extraction', 'sentiment-analysis', 'ner', 'question-answering', 'fill-mask', 'summarization', 'translation_en_to_fr', 'translation_en_to_de', 'translation_en_to_ro', 'text-generation']\"). I guess this is because it is a new feature that hasn’t quite made it to the latest version:\nclassifer = pipeline('zero-shot-classification')\nInstead, I will play around with some of the other classifers.\n\nen_to_de_translate = pipeline('translation_en_to_de')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n/home/simon/.pyenv/versions/3.8.3/envs/hface/lib/python3.8/site-packages/transformers/modeling_auto.py:796: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n  warnings.warn(\n\n\n\n\n\n\n\n\nSome weights of T5ForConditionalGeneration were not initialized from the model checkpoint at t5-base and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\n\nen_to_de_translate(\"no\")\n\n[{'translation_text': 'nein, nein, nein, nein!'}]\n\n\nChecks out. Let’s see if some other stuff accords with my degrading knowledge of German:\n\nen_to_de_translate(\"this is my room\")\n\n[{'translation_text': 'das ist mein Raum.'}]\n\n\nI probably would have used Zimmer instead of Raum, since Raum is more “space” than “room” to me.\n\nen_to_de_translate(\"monkey, hippo, porcupine, dog, cat, rabbit\")\n\n[{'translation_text': 'Affen, Hippo, Pfauen, Hunde, Katzen, Kaninchen, Hunde, Katzen, Kaninchen.'}]\n\n\nIt looks like it uses the plural for nouns. Hippo didn’t translate to anything different, apparently Flusspferd (water horse) is favored by Leo. I like Stachelschwein (“spike pig”) better for porcupine (which apparently live in Texas now!?) and furthermore Pfauen looks to actual mean peacocks. I’m not sure why dog (Hund), cat (Katze), and rabbit (Kaninchen) are repeated, but those look good.\nThus it’s not perfect, but something that took less than a minute can out-translate my 4-ish years of German classes that I haven’t touched up on in like a decade. Ouch."
  },
  {
    "objectID": "posts/old/2020-08-15-huggingface_test.html#named-entity-recognition",
    "href": "posts/old/2020-08-15-huggingface_test.html#named-entity-recognition",
    "title": "Trying Out HuggingFace",
    "section": "Named Entity Recognition",
    "text": "Named Entity Recognition\nFinally, to cap off this short test post let’s try out the named entity recognition task. They provide an example of the classifier in their docs as well as a short list of what different abbreviations mean: * O, Outside of a named entity * B-MIS, Beginning of a miscellaneous entity right after another miscellaneous entity * I-MIS, Miscellaneous entity * B-PER, Beginning of a person’s name right after another person’s name * I-PER, Person’s name * B-ORG, Beginning of an organisation right after another organisation * I-ORG, Organisation * B-LOC, Beginning of a location right after another location * I-LOC, Location\n\nner = pipeline('ner')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFirst using their example:\n\nsequence = (\"Hugging Face Inc. is a company based in New York City. Its headquarters are in DUMBO, therefore very\"\n    + \"close to the Manhattan Bridge which is visible from the window.\")\n\n\nfrom pprint import pprint\n\n\nfor entry in ner(sequence):\n    pprint(entry)\n\n{'entity': 'I-ORG', 'index': 1, 'score': 0.9995632767677307, 'word': 'Hu'}\n{'entity': 'I-ORG', 'index': 2, 'score': 0.9915938973426819, 'word': '##gging'}\n{'entity': 'I-ORG', 'index': 3, 'score': 0.9982671737670898, 'word': 'Face'}\n{'entity': 'I-ORG', 'index': 4, 'score': 0.9994403719902039, 'word': 'Inc'}\n{'entity': 'I-LOC', 'index': 11, 'score': 0.9994346499443054, 'word': 'New'}\n{'entity': 'I-LOC', 'index': 12, 'score': 0.9993270635604858, 'word': 'York'}\n{'entity': 'I-LOC', 'index': 13, 'score': 0.9993864893913269, 'word': 'City'}\n{'entity': 'I-LOC', 'index': 19, 'score': 0.9825621843338013, 'word': 'D'}\n{'entity': 'I-LOC', 'index': 20, 'score': 0.9369831085205078, 'word': '##UM'}\n{'entity': 'I-LOC', 'index': 21, 'score': 0.8987104296684265, 'word': '##BO'}\n{'entity': 'I-LOC',\n 'index': 29,\n 'score': 0.9758240580558777,\n 'word': 'Manhattan'}\n{'entity': 'I-LOC', 'index': 30, 'score': 0.9902493953704834, 'word': 'Bridge'}\n\n\nImpressive, especially how it recognizes DUMBO as a location. (side note, I actually visited that area in my first trip to NYC last year).\nLooking forward to trying out these transformers more in the future!"
  },
  {
    "objectID": "posts/old/2020-07-12-wind_nightmare.html",
    "href": "posts/old/2020-07-12-wind_nightmare.html",
    "title": "Solving a Nightmare with a Headache",
    "section": "",
    "text": "Issue: The widgets in Jupyter Notebook were not all displaying.\nNon-solution: Try re-do some installs in the virtualenv. While I am at it, let’s also try out pyenv.\nNew Issue: Windows does not play well with pyenv. You can get pyenv to work, but then virtualenv is no longer set up on your global environment.\nAttempted Solution: Try to clone the pyenv-virtualenv repo into a new plugins directory of the pyenv root folder. This did not work, despite sinking about an hour into this whole mess.\nNew Issue: It’s time to put Linux on my machine to try and escape some of these Windows nightmares. This is going to be a headache, but it seems like the pay off will be worth-while, given that the Fastbook also doesn’t play well with Windows. I have been frustrated with doing Python things on Windows before so I had Linux available already. I will note that it took be a while to remember I already had a Linux partition and in a cascading wave of failure, I forgot my linux password, so was not able to run a needed sudo command. But, I got lucky again (and this is a yikes security-wise) and it turns out you can easily reset the password for Ubuntu.\nFrom here I followed Real Python’s primer on pyenv. Setting this up was not bad and putting some lines in .bashrc virtualenv becomes much nicer to use. It allows you connect a particular environment to a folder. I created a virtualenv named fab (FastAI Book, short environment names are good when you’re used to activating an environment every time you want to work on something) by again following Real Python and now it is automatically activated when I am in the fastbook directory. A quick pip install -r requirements.txt -v got the environment running smoothly with the notebook for Chapter 1, and things that did not work 100% in Windows worked in my Linux boot nicely (at least after a sudo apt install graphviz): * I could run the cells without having to change num_workers to 0. * My GPU was visible to Torch (torch.cuda.get_device_name(0) returned the name of my GPU). * The text example, which did not work at all on Windows, ran. * And, the whole reason I started this endeavor: when I created the FileUploader widget, it appeared in the notebook."
  },
  {
    "objectID": "posts/misc/cheatsheet.html",
    "href": "posts/misc/cheatsheet.html",
    "title": "Common Command and Setup Cheatsheet",
    "section": "",
    "text": "ctrl + P = previous command\nctrl + u = erase line before cursor\nctrl + l = refresh screen\n\n\n\n\n\nctrl + alt + t = new terminal window\n\n\n\n\n\nsudo apt install vim\nin .vimrc add `:inoremap jk"
  },
  {
    "objectID": "how_to_quarto.html",
    "href": "how_to_quarto.html",
    "title": "ψML",
    "section": "",
    "text": "sudo apt-get install quarto"
  },
  {
    "objectID": "how_to_quarto.html#build",
    "href": "how_to_quarto.html#build",
    "title": "ψML",
    "section": "Build",
    "text": "Build\nquarto preview quarto render"
  }
]