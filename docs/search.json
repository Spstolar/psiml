[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Let’s go."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Psi ML",
    "section": "",
    "text": "jupyter\n\n\n\n\nBeginning the journey of training ML models on sound with rudimentary steps.\n\n\n\n\n\n\nSep 30, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\njupyter\n\n\n\n\nMy first attempt using fastpages to create a post directly from a notebook.\n\n\n\n\n\n\nAug 15, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nfastpages\n\n\n\n\nAwesome description\n\n\n\n\n\n\nAug 9, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmarkdown\n\n\n\n\nA minimal example of using markdown with fastpages.\n\n\n\n\n\n\nAug 2, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmarkdown\n\n\n\n\nA minimal example of using markdown with fastpages.\n\n\n\n\n\n\nAug 1, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nmarkdown\n\n\n\n\nA minimal example of using markdown with fastpages.\n\n\n\n\n\n\nJul 23, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmarkdown\n\n\n\n\nA minimal example of using markdown with fastpages.\n\n\n\n\n\n\nJul 21, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmarkdown\n\n\n\n\nA minimal example of using markdown with fastpages.\n\n\n\n\n\n\nJul 12, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmarkdown\n\n\n\n\nA minimal example of using markdown with fastpages.\n\n\n\n\n\n\nJul 11, 2020\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "how_to_quarto.html",
    "href": "how_to_quarto.html",
    "title": "Psi ML",
    "section": "",
    "text": "quarto preview quarto render"
  },
  {
    "objectID": "posts/2020-08-02-Python_and_MIDI_2.html",
    "href": "posts/2020-08-02-Python_and_MIDI_2.html",
    "title": "Make More Interesting Random Music",
    "section": "",
    "text": "We saw last time that it is simple to construct a MIDI file with the pretty_midi package. Now to make something a little more musically interesting than alternating between two notes we need to randomly generate notes to play using some basic music theory to make things sound “good.” To do this we will:\n\nchoose a scale\nfind all of the midi notes attached to that scale\nrandomly draw notes from that collection of midi notes\nget a pleasing collection of simple chords to accompany the melody\nvary the rhythm of the melody\n\n\n\nFirst we use a couple of nice utilities of pretty_midi, a method which takes an integer and spits out one of the major and minor keys and another method which takes that same integer and returns how many accidentals the key has. Using the key name we can determine whether the key has flat accidentals or sharp accidentals and then by using the fact that “accidentals accumulate” (if a key has a G# then it also has a C#, for example) we can easily identify which notes are in the key.\nTo say more, we are using the fact that the Major and Minor keys have certain nice patterns. If we start from C and move up in perfect 5ths (C, G, D, A) then those corresponding (major) keys each add an accidental:\n\nC Major: CDEFGAB\nG Major: GABCDEF#\nD Major: DEF#GABC#\nA Major: ABC#DEF#G#\n…\n\nThis means we know the exact notes that are made sharp or (flat) if we know if the key is sharp (flat) and how many accidentals there are, we do not even need to know the root: we are being told the same information in a different way. Now, there is actually a more insightful way to do this (how making major scales is normally taught) by using the fact that you start at the root and add notes with the pattern WWHWWWH, but it was fun to think of a different way to get the notes.\nWe return the key_notes which runs from C to B because this is how the MIDI format is laid out for each octave (… B1 C2 C#2 … A#2 B2 C3 …), but we also save off the notes of the key starting from the root for (optional) printing to the user as scale_notes.\nnote_names = [n for n in 'CDEFGAB']\nsharp_accidentals = [n for n in 'FCGDAEB']\nflat_accidentals = [n for n in 'BEADGCF']\n\ndef determine_key_notes(key_number):\n    key_name = pretty_midi.key_number_to_key_name(key_number)\n    _, num_accidentals = pretty_midi.key_number_to_mode_accidentals(key_number)\n    root = key_name[0]\n    root_index = note_names.index(root)\n\n    if key_name[1] == \"b\":\n        accidental_mark = 'b'\n        accidentals = flat_accidentals[:num_accidentals]\n    else:\n        accidental_mark = '#'\n        accidentals = sharp_accidentals[:num_accidentals]\n\n    key_notes = list(map(lambda n: n + accidental_mark if n in accidentals else n, note_names))\n\n    scale_notes = (key_notes + key_notes)[root_index:root_index + 7]\n\n\n\nFor this, use the pretty_midi utility that converts note names to MIDI note numbers and apply it to all the note names with all the octave numbers attached.\ndef get_all_midi_numbers(note_names):\n    midi_numbers = []\n    note_names = list(set(note_names))  # simple de-dup\n    for octave in range(-1, 9):\n        for note in note_names:\n            midi_number = pretty_midi.note_name_to_number(note + str(octave))\n            midi_numbers.append(midi_number)\n    return sorted(midi_numbers)\nWe do not need to de-duplicate or sort for our current use, but I added those steps in case I supply some note names “out of order” ([D, C] instead of [C, D] for instance) or provide possible note duplicates for other uses.\n\n\n\nHere you can now use the collection of midi notes and just make a random choice from it at each step. For instance if g_maj_midi is the collection of MIDI notes for G Major then you can randomly select one with pitch = random.choice(g_maj_midi[21:36]) where we take a slice of the array to restrict the notes to just a couple of octaves.\n\n\n\nWe want to get just simple chords from the major scale for the piano to play them for whole notes. There are some existing Python packages that can be used:\n\nchords2midi let’s you generate a MIDI file by supplying a progressing an a key: c2m I V vi IV --key C\nchords2midi uses pychord, which will generate the component notes of a chord from its name as well as name a chord from its notes.\n\nThese are both strong utilities that I will certainly use as I expand but for now I will do something much simpler. You can get a major scale chord progression by simply going to the scale and taking a note for the root and getting the third and fifth of a triad by just taking the second and fourth notes after your root. For instance, you can get a C (major) triad from the C major scale by looking at the scale CDEFGAB and using that pattern C_E_G__. Whether this is major or minor is irrelevant for our use: we just want to grab all the simple triads (not worrying about inversions or anything) and collect them together for one octave of root notes:\ndef get_major_progression(root_index, midi_numbers):\n    chords = []\n    for i in range(8):\n        chord_root_index = root_index + i\n        chord_third_index = chord_root_index + 2\n        chord_fifth_index = chord_root_index + 4\n        root = midi_numbers[chord_root_index]\n        third = midi_numbers[chord_third_index]\n        fifth = midi_numbers[chord_fifth_index]\n        chord = [root, third, fifth]\n        chords.append(chord)\n    return chords\nThis will generate a list of chord lists, where each chord list is the midi notes for a given triad. Note, if you do not feed the notes of a key as the MIDI numbers you will not get a major key progression, so this might be imperfectly named.\n\n\n\nWhen we were adding notes one at a time we were keeping track of when the note began and when it ended in seconds. It would be nice to forget about when they begin and imagine writing the song and moving forward, adding notes “now.” To do this we make a writer for our instrument that keeps track of when “now” is and allows us to add notes one at a time, as a chord, or in a chunk of notes. Using this chunking allows us to vary the rhythm, we can randomly determine how long the next note(s) should be and then play a bunch of notes of that length. This is slightly more natural than varying each note independently, because shorter notes are often grouped together.\nclass Writer:\n    def __init__(self, instrument):\n        self.position = 0\n        self.instrument = instrument\n\n    def add_note(self, pitch, length, move_forward=True):\n        note = pretty_midi.Note(velocity=100, pitch=pitch, start=self.position, end=self.position + length)\n        self.instrument.notes.append(note)\n\n        if move_forward:\n            self.position += length\n\n    def add_note_series(self, pitches, length):\n        for pitch in pitches:\n            self.add_note(pitch, length)\n\n    def add_chord(self, pitches, length, move_forward=True):\n        for pitch in pitches:\n            self.add_note(pitch, length, move_forward=False)\n        if move_forward:\n            self.position += length\nWe leave move_forward as an optional argument, because if we are writing notes that will occur at the same time (to form a chord) then we want the Writer “head” or position to stay at the same spot until we are done adding notes to that point in time. There are other simplifications that are made (no velocity changes), but this simple class gives us a lot of power so that we can more expressively generate midi music.\nimport pretty_midi\nfrom music_info import determine_key_notes\nimport music_info\nimport random\n\n# Create a PrettyMIDI object\nensemble = pretty_midi.PrettyMIDI()\n\n# Create an Instrument instance for a cello instrument\n# Changed to a guitar for my song, and was lazy about changing variable names\n# cello_program = pretty_midi.instrument_name_to_program('Cello')\ncello_program = pretty_midi.instrument_name_to_program('Overdriven Guitar')\ncello = pretty_midi.Instrument(program=cello_program)\n\n# do the same for a piano\npiano_program = pretty_midi.instrument_name_to_program('Acoustic Grand Piano')\npiano = pretty_midi.Instrument(program=piano_program)\n\n# Add the instruments to the PrettyMIDI object\nensemble.instruments.append(cello)\nensemble.instruments.append(piano)\n\n# here is where I put the Writer class from above\n\npiano_writer = Writer(piano)\ncello_writer = Writer(cello)\n\nsong_length_in_seconds = 30\nbpm = 120\nbeat_length = 60 / bpm\nnum_beats = int(song_length_in_seconds / beat_length)\n\n# Decided on G major, which is index 7\ng_maj = music_info.determine_key_notes(7)\ng_maj_midi = music_info.get_all_midi_numbers(g_maj)\nroot_number = pretty_midi.note_name_to_number(\"G4\")\nroot_index = g_maj_midi.index(root_number)\nmajor_progression = music_info.get_major_progression(root_index, g_maj_midi)\n\naccompaniment_writer = piano_writer\nsolo_writer = cello_writer\n\n# now we see the power of the writer object\nwhile accompaniment_writer.position < song_length_in_seconds:\n    length = beat_length * 4  # whole note chords\n\n    # choose a random chord from the progression\n    chord = random.choice(major_progression)  \n\n    # add a lower octave of the notes for fullness\n    larger_chord = chord + [n - 12 for n in chord]\n\n    # write the chord\n    accompaniment_writer.add_chord(larger_chord, length)\n\nwhile solo_writer.position < song_length_in_seconds:\n    # choose to play 16th, 8th, quarter, half, or whole note(s)\n    division = random.choice([-2, -1, 0, 1, 2])\n    length = beat_length * (2 ** division)\n\n    # if we chose 16th or 8th, play multiple of them\n    if division < 0:\n        num_notes = 2 ** (-1 * division)\n        pitches = random.choices(g_maj_midi[21:36], k=num_notes)\n        solo_writer.add_note_series(pitches, length)\n    else:\n        pitch = random.choice(g_maj_midi[21:36])\n        solo_writer.add_note(pitch, length=beat_length)\n\ndef write_song():\n    # Write out the MIDI data\n    ensemble.write('duo.mid')\nNow with this I created the following simple duet. It isn’t the most thrilling piece of music, but with relatively simple rules behind it, I think it is interesting how “complex” it sounds."
  },
  {
    "objectID": "posts/2020-09-30-reading_in_a_wave_file.html",
    "href": "posts/2020-09-30-reading_in_a_wave_file.html",
    "title": "Reading in a Wave File",
    "section": "",
    "text": "The sound I created was simple 2 seconds of a sine wave at around C4. Zooming in to a 0.045 second window we can see the wave in Reaper. A simple peak-to-peak measure was 0.00375 seconds long which gives a frequency around 267 herz. Since this was a C4, which is typically 262 it is not too far off.\nTo read the file a quick search brought up the wavfile method from scipy. I basically follow the example provided there on my sound.\n\nfrom scipy.io import wavfile\nfrom pathlib import Path\n\n\np = Path('sounds')\n\n\nq = p / 'simple_c4.wav'\n\n\nq\n\nWindowsPath('sounds/simple_c4.wav')\n\n\n\nsample_rate, data = wavfile.read(q)\n\nc:\\python37\\lib\\site-packages\\scipy\\io\\wavfile.py:273: WavFileWarning: Chunk (non-data) not understood, skipping it.\n  WavFileWarning)\n\n\n\nsample_rate\n\n44100\n\n\n\ndata\n\narray([[ 0.0000000e+00,  0.0000000e+00],\n       [ 0.0000000e+00,  0.0000000e+00],\n       [-3.8028106e-06, -3.8028106e-06],\n       ...,\n       [-6.6743125e-03, -6.6743125e-03],\n       [-4.4752425e-03, -4.4752425e-03],\n       [-2.2473824e-03, -2.2473824e-03]], dtype=float32)\n\n\n\ndata.shape\n\n(88200, 2)\n\n\nSince the recording is 2 seconds long and the sample rate is 44100, there are 88200 samples total, as expected. This is a list of channel lists. So, the stereo data is recorded for each sample.\n\nlength = data.shape[0] / sample_rate\nlength\n\n2.0\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ntime = np.linspace(0., length, data.shape[0])\n\nIn this case the channels are the same, so just plotting the first one:\n\nplt.plot(time, data[:, 0])\nplt.xlabel(\"Time [s]\")\nplt.ylabel(\"Amplitude\")\nplt.show()\n\n\n\n\nToo much info for this tiny graph! Let’s look at just one second and then zoom in more.\n\nspan_length = data.shape[0] // 2\n\n\nspan_length\n\n44100\n\n\n\nplt.plot(time[:span_length], data[:span_length, 0])\nplt.xlabel(\"Time [s]\")\nplt.ylabel(\"Amplitude\")\nplt.show()\n\n\n\n\n\nspan_length = data.shape[0] // 20\n\n\nspan_length\n\n4410\n\n\n\nplt.plot(time[:span_length], data[:span_length, 0])\nplt.xlabel(\"Time [s]\")\nplt.ylabel(\"Amplitude\")\nplt.show()\n\n\n\n\n\nspan_length = data.shape[0] // 200\n\n\nspan_length\n\n441\n\n\n\nstart_sample = 500\n\n\nplt.plot(time[start_sample:start_sample+span_length], data[start_sample:start_sample+span_length, 0])\nplt.xlabel(\"Time [s]\")\nplt.ylabel(\"Amplitude\")\nplt.show()\n\n\n\n\nA simple sine wave!"
  },
  {
    "objectID": "posts/2020-07-11-fastai_book_ch2.html",
    "href": "posts/2020-07-11-fastai_book_ch2.html",
    "title": "FastAI Book Chapter 2",
    "section": "",
    "text": "I went through the second chapter of the book today, which is why this blog even exists. Highly useful; looking forward to working on some music projects to really learn the material.\n\n\n\nHow to easily build an image classifier to discern between blue jays, mockingbirds, and shrikes.\n\nI chose the first two bird types based on my familiarity and their Cool Local Bird ranks. I was going to select a cardinal as a third class, but thought it would be too easy to detect the class based solely on color, so I searched around and found shrikes, which looked similar to mockingbirds. The classifier performed very well, which was I semi-shock because I didn’t know:\n\nYou do not always need a lot of data to build a decent model.\n\nEven with 150 examples of each of three classes (before train/validation split), there were only 3 misclassifications on the validation set. This shows how powerful data augmentation can be, but also helps dispel the notion that you need tons of data to do anything reasonable.\n\nFastAI has some powerful tools that I need to learn.\n\n\nEasily display the examples with the highest loss and lowest confidence to see if I can interpret possible deficiencies with interp.plot_top_losses().\nClean up the dataset with ImageClassifierCleaner, manually going through some of the examples to change labels or remove them, and then effect the changes with a couple simple for-loops.\nUse verify_images to clean up corrupted files easily.\nUse DataBlock to define the structure of the problem and implement useful transformations for the data.\n\n\nThink about how to normalize images.\n\nSquashing a large image into a smaller one with simple scaling or adding cropped parts of small images is maybe not the best way to make sure the inputs are all “from the same distribution”. You can use RandomResizedCrop to maintain original image quality and also artificially expand the size of your training set.\n\nOther things:\n\nHow to easily build a dataset with the Bing Image API.\nThe usefulness of Path (which I only recently learned about) was really demonstrated nicely.\nThe Drivetrain Approach.\nCreating a Notebook App with widgets and Voilà. (Admittedly, I’m still trying to get the latter to work.)\n\n\n\n\n\n\nHow do you find the Bing Image Search key?\n\nSolution: I signed up for the free Azure thing, went to to the Bing Image Search API, and just had to click to add Bing Search APIs v7 to my subscription. Once I did this it brought up a page with the keys and endpoints.\n\nI am using my local computer, with Windows, to run the notebooks. So, I have run into (standard) issues. Namely, I saw RuntimeError: cuda runtime error (801) : operation not supported at... when I first attempted to fine tune the learner learn.fine_tune(4).\n\nSolution: When you google the error this issue page points you to the forum, but also usefully mentions the “need to set num_workers=0 when creating a DataLoaders because Pytorch multiprocessing does not work on Windows.” So, doing this when you define the dataloaders dls = bears.dataloaders(path, num_workers=0) cleared it up for me.\n\nSome minor formatting issues with interp.plot_top_losses(5, nrows=1). The text above the images was overlapping, because my class names were a bit long.\n\nSimple fix was to set nrows=5."
  },
  {
    "objectID": "posts/2020-08-01-Python_and_MIDI.html",
    "href": "posts/2020-08-01-Python_and_MIDI.html",
    "title": "Making Music with pretty_midi",
    "section": "",
    "text": "A while back, when I was first learning Python, a friend and I made a program that generated random music by iterating through a song one note at a time randomly picking the note length and pitch for the notes with increasingly strict rules. My friend figured out how to use MIDIUtil to create a midi file by writing down a sequence of note events, where each event says something about:\n\nvelocity - how loud the note should be played\npitch - an integer value for telling a midi play what frequency to play\nnote start - when the note should start playing\nnote end - when it should stop\n\nYou can do a lot of music with just that information, and if you make simple choices for notes, with the right MIDI player you can get surprisingly listenable music. I have wanted to expand on this program for a while in various ways:\n\ngenerating music by applying machine learning techniques to various collections of midi files (like this Google Bach doodle)\nmaking something more interactive (something involving the console where you can add motifs and ideas on the play and have them played back)\nfeed the program just a text file with a simplistic music notation to easily create ideas that can get more complex (like TidalCycles which uses Haskell and SuperCollider).\n\n\n\n\nI searched around on pypi a bit and found a promising package to start writing MIDI files with: pretty_midi. It’s a project on GitHub that seems to still be active (part of why I am not just using MIDIUtil again is that I remember I slightly annoying setup and it seems to be inactive). After a simple install, and running the second example from the documentation I found that it easily generated pleasing sounds that (after a bit of fitzing) I could simply listen to with VLC Media Player. Before I just imported the files into Reaper where I could use a custom VST, but this makes iterating a bit quicker.\nHere is an example I made where two instruments are playing a very minimalist piece:\nimport pretty_midi\n\n# Create a PrettyMIDI object\nensemble = pretty_midi.PrettyMIDI()\n\n# Create an Instrument instance for a cello instrument\ncello_program = pretty_midi.instrument_name_to_program('Cello')\ncello = pretty_midi.Instrument(program=cello_program)\n\n# do the same for a piano\npiano_program = pretty_midi.instrument_name_to_program('Acoustic Grand Piano')\npiano = pretty_midi.Instrument(program=piano_program)\n\n# Add the instruments to the PrettyMIDI object\nensemble.instruments.append(cello)\nensemble.instruments.append(piano)\n\n\nsong_length_in_seconds = 30\nbpm = 120\nbeat_length = 60 / bpm\nnum_beats = int(song_length_in_seconds / beat_length)\n\nfor beat in range(num_beats):\n    if beat % 2 == 0:\n        note_name = 'C5'\n    else:\n        note_name = 'D5'\n    note_number = pretty_midi.note_name_to_number(note_name)\n    note_start = beat * beat_length\n    note_end = note_start + beat_length\n    note = pretty_midi.Note(velocity=100, pitch=note_number, start=note_start, end=note_end)\n    cello.notes.append(note)\n    piano.notes.append(note)\n\n\ndef write_song():\n    # Write out the MIDI data\n    ensemble.write('ensemble.mid')\n\nif __name__ == \"__main__\":\n    write_song()\nThis is a simple example, yet it shows the basics of the process that I will expand on:\n\nadd notes one at a time\nincorporate some randomness into the note properties (here pitch changes deterministically, but that will soon change)\nusing multiple instruments\n\nI will begin to add to this, hopefully reaching what we achieved during my first experience with Python and MIDI and then think about ways to expand it and then start to apply some of my fastbook reading to it."
  },
  {
    "objectID": "posts/2022-02-12-cheatsheet.html",
    "href": "posts/2022-02-12-cheatsheet.html",
    "title": "Psi ML",
    "section": "",
    "text": "ctrl + P = previous command\nctrl + u = erase line before cursor\n\n\n\n\n\nctrl + alt + t = new terminal window\n\n\n\n\n\nsudo apt install vim\nin .vimrc add `:inoremap jk"
  },
  {
    "objectID": "posts/2020-07-12-wind_nightmare.html",
    "href": "posts/2020-07-12-wind_nightmare.html",
    "title": "Solving a Nightmare with a Headache",
    "section": "",
    "text": "Issue: The widgets in Jupyter Notebook were not all displaying.\nNon-solution: Try re-do some installs in the virtualenv. While I am at it, let’s also try out pyenv.\nNew Issue: Windows does not play well with pyenv. You can get pyenv to work, but then virtualenv is no longer set up on your global environment.\nAttempted Solution: Try to clone the pyenv-virtualenv repo into a new plugins directory of the pyenv root folder. This did not work, despite sinking about an hour into this whole mess.\nNew Issue: It’s time to put Linux on my machine to try and escape some of these Windows nightmares. This is going to be a headache, but it seems like the pay off will be worth-while, given that the Fastbook also doesn’t play well with Windows. I have been frustrated with doing Python things on Windows before so I had Linux available already. I will note that it took be a while to remember I already had a Linux partition and in a cascading wave of failure, I forgot my linux password, so was not able to run a needed sudo command. But, I got lucky again (and this is a yikes security-wise) and it turns out you can easily reset the password for Ubuntu.\nFrom here I followed Real Python’s primer on pyenv. Setting this up was not bad and putting some lines in .bashrc virtualenv becomes much nicer to use. It allows you connect a particular environment to a folder. I created a virtualenv named fab (FastAI Book, short environment names are good when you’re used to activating an environment every time you want to work on something) by again following Real Python and now it is automatically activated when I am in the fastbook directory. A quick pip install -r requirements.txt -v got the environment running smoothly with the notebook for Chapter 1, and things that did not work 100% in Windows worked in my Linux boot nicely (at least after a sudo apt install graphviz): * I could run the cells without having to change num_workers to 0. * My GPU was visible to Torch (torch.cuda.get_device_name(0) returned the name of my GPU). * The text example, which did not work at all on Windows, ran. * And, the whole reason I started this endeavor: when I created the FileUploader widget, it appeared in the notebook."
  },
  {
    "objectID": "posts/2020-07-23-data_objects.html",
    "href": "posts/2020-07-23-data_objects.html",
    "title": "DataPlox",
    "section": "",
    "text": "DataBlock\nThis is a fastai object which helps you build Datasets and DataLoaders. In addition to Chapter 6 of the fastbook, there is also a tutorial in the fastai docs. A DataBlock is a blueprint for how to build your data. You tell it:\n\nwhat kind of data you have (blocks=),\nhow to get the input data (get_x= or get_items=),\nhow to get the targets/labels (get_y),\nhow to perform the train/validation split (splitter=),\nas well as any resizing (items_tfms=) or augmentations you want to be performed (batch_tfms).\n\nBy feeding these blue prints a source (like a directory on your computer), you can use a DataBlock to create a Datasets or DataLoaders object.\n\n\nDataset\nDataset is a Torch object. We can find out exactly what a Dataset is because PyTorch is open source. We just have to be brave enough to parse some of the grittier implementation details.\nAccording to the source code a Dataset is at its core something that allows you to grab an item if you provide the index/key for it and that you can also add items to. This is just the abstract class definition, essentially the bare bones of a what a dataset should be. If you try to make a class that inherits from Dataset you will get an error if you do not implement __getitem__, the method for grabbing items. It does this by setting the default behavior of that method to raise a NotImplementedError. You can also implement this behavior (forcing inheriting classes to define specific methods) by using the abc package. The source code also mentions that it would have set a default for a length function, but the standard methods for making a default that is forced to change have conflicts with what a length function is “supposed” to do.\nThe types of Datasets are:\n\nIterableDataset\nTensorDataset\nConcatDataset\nChainDataset\nSubset\n\n\n\nDatasets\nDatasets is an object that contains a training Dataset and a validation Dataset. You can generally construct a Datasets object from a DataBlock like this:\ndblock = DataBlock(blue_print_details)\ndsets = dblock.datasets(source)\n\n\nDataLoader\nA DataLoader is a Dataset together with a Sampler. A Sampler is a way to create an iterator out of your Dataset, so you can do things like consume data in batches as needed. Rather than take a Dataset an manually loop through chunks of it, at each step using a chunk to update a model, a DataLoader bundles this idea together. This makes a lot of sense to encapsulate: going through your data in batches is a frequently encountered process in machine learning!\nWe can see from the source code that a Sampler is at minimum:\n\na way to iterate over indices of dataset elements (__iter__) and\na way to calculate the size of the returned iterators (__len__).\n\nJust like with DataSet, defining the length method is not strictly enforced by the interpreter because the various NotImplemented errors you can throw do not quite work.`\nThe kinds of samplers:\n\nSequentialSampler - go in direct 0, 1, 2, … order.\nRandomSampler - randomly choose observations, with or without replacement (replacement=)\nSubsetRandomSampler - randomly sample from a provided subset of indices, without replacement\nWeightedRandomSampler - for non-uniform random sampling\nBatchSampler - generate mini-batches of indices\n\nFor DataLoader, the definition is a bit more involved. In part, because it implements multiprocessing, but it also does things like creating a Sampler from the arguments if one wasn’t provided.\n\n\nDataLoaders\nDataLoaders is an object that contains a training DataLoader and a validation DataLoader. You can construct a DataLoaders from a DataBlock similarly to the Datasets method:\ndblock = DataBlock(blue_print_details)\ndls = dblock.dataloaders(source)"
  },
  {
    "objectID": "posts/2020-07-21-fastbook_ch5.html",
    "href": "posts/2020-07-21-fastbook_ch5.html",
    "title": "FastBook Chapter 5 Thoughts",
    "section": "",
    "text": "After reading the first half of chapter 3 (will read the second half this week) and (mostly) breezing through Chapter 4 (it contained a lot of familiar material), I worked on Chapter 5 this weekend.\n\n\n\nPresizing.\nChecking your DataBlock before you begin training\nTrain early (get a reasonable MVP) and often (if it’s not too expensive).\nCross-Entropy Loss for the binary case and extending it to multi-class examples.\nConfusion matrix with ClassificationInterpretation and looking at the most_confused examples.\nLearning Rate Finder\nMore particulars on transfer learning, including how to use discriminative learning rates to not lose the solid training of the transferred modeled.\n\n\n\n\n\nPresizing is a particular way to do image augmentation that is designed to minimize data destruction while maintaining good performance. The general idea is to apply a composition of the augmentation operations all at once rather than iteratively augment and interpolate. This has savings both in terms of computation and the final quality of the examples.\n\nIn the 3s and 7s table there is a column labeled “loss”, which for me was a bit confusing. In the first row loss was the predicted output of the “3” class, which happened to be the correct answer. However, loss was just the output of that example, which does not quite make sense because you are looking to minimize loss which conflicts with the goal of maximizing the predicted output for the true class. It looks like this was just an oversight with the naming convention because to compute the loss more things are done and the text that follows makes this clear.\nI found it useful to explicitly calculate the loss in the binary example provided.\nActivations:\nacts[0, :]\n\ntensor([0.6734, 0.2576])\n\nclass0_act = acts[0, 0]\nclass1_act = acts[0, 1]\nclass0_act\n\ntensor(0.6734)\n\nComputing the exponential of the activations to then get the softmax.\nfrom math import exp\nexp0 = exp(class0_act)\nexp0\n\n1.9608552547588787\n\nexp1 = exp(class1_act)\nsmax0 = exp0 / (exp0 + exp1)\nsmax0\n\n0.602468670578454\n\nsmax1 = exp1 / (exp0 + exp1)\nsmax1\n\n0.39753132942154595\n\nsmax0 + smax1\n\n1.0\n\nfrom math import log\nlog(smax0)\n\n-0.5067196140092344\n\nlog(smax1)\n\n-0.9224815318387478\n\n-log(smax0)\n\n0.5067196140092344\n\nAnd that is the loss for the first example, because the true class was 0. This matches the calculation using the fastai classes, which is always a relief.\n\n\n\n\nCyclical Learning Rates for Training Neural Networks\nHow transferable are features in deep neural networks?\n\n\n\n\n\nWhy do we first resize to a large size on the CPU, and then to a smaller size on the GPU?\nYou want to create a uniform input size for your data and also apply various transformations to augment it. The presizing method, running augmentation transformations as a single composition rather than iteratively, allows you to have larger/more “rich” inputs to transform before making them a smaller, uniform size that you will train the model with.\nWhat are the two ways in which data is most commonly provided, for most deep learning datasets?\n\nA collection of data elements, that have filenames indicating information about them, like their class. (A folder of pictures where each picture has a file name with its ID and class).\nTabular format that can either contain the data in each row (along with the metadata) or point to data in other formats. (A csv file with ID, true class, and a hyper link to the input picture.)\n\nLook up the documentation for L and try using a few of the new methods is that it adds.\nL is a beefier list class. How it’s different from a regular list:\n\nthe print function is smarter. It provides the length of the list and truncates the end, which is nice if you’ve ever crashed a server because you accidentally printed out an obscenely long list.\nyou can access L with a tuple, whereas a normal list will break if you try to access it that way.\nit has unique(), which functions like the same method in Pandas.\nit has a filter method attribute.\n\nLook up the documentation for the Python pathlib module and try using a few methods of the Path class.\nPath was introduced to Python in 3.4. It appears to combine a bunch of common things that you typically use os with along with the ability to manage file paths without doing string manipulations (as well as reducing the \\ vs / mistakes that are frequently made).\nOne nice thing that can be done, set here = Path('.') and then iterate over the current directory with for f in here.iterdir(): print(f). You can also .open() a path object rather than feeding it to open() and do glob stuff.\nGive two examples of ways that image transformations can degrade the quality of the data.\n\nImage simply rotating a square 45 degrees to stand it up on one corner. Now the new image that you get (take an old square position cut out of the rotated square position) is missing anything in the corners, so it has to be interpolated. This loses about 17% of the original image, so it’s pretty significant!\nBrightening an image will move the brighter pixels up to the maximum brightness, so their original brightness cannot be recovered by simply redarkening.\n\nWhat method does fastai provide to view the data in a DataLoaders?\nYou can use .show_batch(nrows, ncols) on the DataLoaders object to get a grid of some of the examples.\nWhat method does fastai provide to help you debug a DataBlock?\nUsing .summary() on the DataBlock object gives a verbose attempt to try and create the batch. The output from this, along with errors that come up if it fails can help you notice a problem.\nShould you hold off on training a model until you have thoroughly cleaned your data?\nNo, sometimes life is easy! Also, it’s good to get reasonable bench marks as soon as possible. Not only to help game-ify the problem and motivate you to work on it, but also to have a baseline to see if the room for improvement is worth the energy.\nWhat are the two pieces that are combined into cross-entropy loss in PyTorch?\nnn.CrossEntropyLoss (see the docs) applies nll_loss after log_softmax (which is log of softmax)\nWhat are the two properties of activations that softmax ensures? Why is this important?\n\nYou can interpret activations as probabilities.\n\noutputs sum to one\noutputs are non-negative\n\nIt forces the model to favor a single class.\n\nThis more relevant behavior that is mentioned that it amplifies small differences, which is useful if you want the network to be somewhat decisive rather than having all outputs close to each other.\nWhen might you want your activations to not have these two properties?\nThe parenthetical comment in the main text mentions that you may not want the model to pick a class just because it has a slightly larger output. You want the model to be sure about the class, not just relatively sure.\nFor the probability property, it might be misleading because it isn’t necessarily the actual probability of the example being that class.\nWhy can’t we use torch.where to create a loss function for datasets where our label can have more than two categories?\nIn part, this is a constraint of the where function. Where selects between two outputs based on a condition. It is too difficult to right a nested condition when you have more than two outcomes and selecting the loss requires a bit more work, so this trick becomes way less convenient.\nWhat are two good rules of thumb for picking a learning rate from the learning rate finder?\n\n\nOne order of magnitude less than where the minimum loss was achieved (i.e., the minimum divided by 10).\nThe last point where the loss was clearly decreasing\n\n\nWhat two steps does the fine_tune method do?\nWe go to the source:\nself.freeze()\nself.fit_one_cycle(freeze_epochs, slice(base_lr), pct_start=0.99, **kwargs)\nbase_lr /= 2\nself.unfreeze()\nself.fit_one_cycle(epochs, slice(base_lr/lr_mult, base_lr), pct_start=pct_start, div=div, **kwargs)\nThus it: 1. Performs a one-cycle fit with the pre-trained layers frozen (their weights do not update). 2. Performs another one-cycle fit with the pre-trained layers unfrozen at half the learning rate.\nWhat are discriminative learning rates?\nThe idea here is that you may want weights in certain layers to change at a different rate. In particular, if your first layers come from a pretrained network you may want to do updates to them more slowly than the last layers which are tailored to your particular problem.\nHow is a Python slice object interpreted when passed as a learning rate to fastai?\nIt acts like numpy.linspace where the num is implicitly defined as the number of layers.\nWhy is early stopping a poor choice when using 1cycle training?\nFor a description of 1cycle training, the fastai docs refer to the rate finder paper in the references as well as this blog post. It looks like the basic idea is stopping early does give the training a chance to be finely tuned, because you are likely stopping at a point where the learning rate is still large.\nWhat is the difference between resnet50 and resnet101?\nBoth resnet50 and resnet100 are residual networks, and seem to have been introduced in Deep Residual Learning for Image Recognition. The basic idea of deep residual networks seems to be “wire a network that is trying to learn the function \\(\\mathcal{H}(x)\\) such that it has to learn \\(\\mathcal{H}(x) - x\\) instead.” The intuition being that, for example, it is easier to learn the zero function than it is to learn the identity function. resnet-50 looks like it was obtain from the resnet-34 architecture by replacing certain layer pairs with layer triplets known as bottleneck blocks. resnet-101 (and resnet-152) are just an expansion of this idea, adding 17 more (or 34 more) of these triplet-layer blocks.\nWhat does to_fp16 do?\n\nThe other downside of deeper architectures is that they take quite a bit longer to train. One technique that can speed things up a lot is mixed-precision training. This refers to using less-precise numbers (half-precision floating point, also called fp16) where possible during training. As we are writing these words in early 2020, nearly all current NVIDIA GPUs support a special feature called tensor cores that can dramatically speed up neural network training, by 2-3x. They also require a lot less GPU memory. To enable this feature in fastai, just add to_fp16() after your Learner creation (you also need to import the module)."
  },
  {
    "objectID": "posts/2020-08-15-huggingface_test.html",
    "href": "posts/2020-08-15-huggingface_test.html",
    "title": "Trying Out HuggingFace",
    "section": "",
    "text": "After going through the pain of converting a Notebook to a markdown file and then editing that markdown file to look nice (in my last post), I saw that there was a better way to hand that process: fastpages. The process was slightly rocky, but I finally think I have things more or less figured out, including linking it to a domain under my name!\nAs a first test of the capability of uploading a notebook to a blog post, I am going to toy with the Hugging Face models. Interesting name for a company/group, with lots of Alien vibes. I saw this super cool tweet: > twitter: https://twitter.com/huggingface/status/1293240692924452864?s=20\nPer the instructions here I made a virtual environment to try out some transformers:\npyenv virtualenv 3.8 hface\npyenv activate hface\npip install jupyter\npip install --upgrade pip\npip install torch\npip install transformers\nAnd then I tested with:\npython -c \"from transformers import pipeline; print(pipeline('sentiment-analysis')('I hate you'))\"\nWhich gave a correct sentiment score, I think at least, a negative score close to 1.\n\nfrom transformers import pipeline\n\n\npipeline('sentiment-analysis')('jog off')\n\n[{'label': 'NEGATIVE', 'score': 0.905813992023468}]\n\n\n\npipeline('sentiment-analysis')('exactly')\n\n[{'label': 'POSITIVE', 'score': 0.9990326166152954}]\n\n\n\npipeline('sentiment-analysis')('I saw this super cool tweet')\n\n[{'label': 'POSITIVE', 'score': 0.998775064945221}]\n\n\nVery cool!"
  },
  {
    "objectID": "posts/2020-08-15-huggingface_test.html#trying-out-pipelines",
    "href": "posts/2020-08-15-huggingface_test.html#trying-out-pipelines",
    "title": "Trying Out HuggingFace",
    "section": "Trying Out Pipelines",
    "text": "Trying Out Pipelines\nI attempted running a zero-shot classifier, but got an error (\"Unknown task zero-shot-classification, available tasks are ['feature-extraction', 'sentiment-analysis', 'ner', 'question-answering', 'fill-mask', 'summarization', 'translation_en_to_fr', 'translation_en_to_de', 'translation_en_to_ro', 'text-generation']\"). I guess this is because it is a new feature that hasn’t quite made it to the latest version:\nclassifer = pipeline('zero-shot-classification')\nInstead, I will play around with some of the other classifers.\n\nen_to_de_translate = pipeline('translation_en_to_de')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n/home/simon/.pyenv/versions/3.8.3/envs/hface/lib/python3.8/site-packages/transformers/modeling_auto.py:796: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n  warnings.warn(\n\n\n\n\n\n\n\n\nSome weights of T5ForConditionalGeneration were not initialized from the model checkpoint at t5-base and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\n\nen_to_de_translate(\"no\")\n\n[{'translation_text': 'nein, nein, nein, nein!'}]\n\n\nChecks out. Let’s see if some other stuff accords with my degrading knowledge of German:\n\nen_to_de_translate(\"this is my room\")\n\n[{'translation_text': 'das ist mein Raum.'}]\n\n\nI probably would have used Zimmer instead of Raum, since Raum is more “space” than “room” to me.\n\nen_to_de_translate(\"monkey, hippo, porcupine, dog, cat, rabbit\")\n\n[{'translation_text': 'Affen, Hippo, Pfauen, Hunde, Katzen, Kaninchen, Hunde, Katzen, Kaninchen.'}]\n\n\nIt looks like it uses the plural for nouns. Hippo didn’t translate to anything different, apparently Flusspferd (water horse) is favored by Leo. I like Stachelschwein (“spike pig”) better for porcupine (which apparently live in Texas now!?) and furthermore Pfauen looks to actual mean peacocks. I’m not sure why dog (Hund), cat (Katze), and rabbit (Kaninchen) are repeated, but those look good.\nThus it’s not perfect, but something that took less than a minute can out-translate my 4-ish years of German classes that I haven’t touched up on in like a decade. Ouch."
  },
  {
    "objectID": "posts/2020-08-15-huggingface_test.html#named-entity-recognition",
    "href": "posts/2020-08-15-huggingface_test.html#named-entity-recognition",
    "title": "Trying Out HuggingFace",
    "section": "Named Entity Recognition",
    "text": "Named Entity Recognition\nFinally, to cap off this short test post let’s try out the named entity recognition task. They provide an example of the classifier in their docs as well as a short list of what different abbreviations mean: * O, Outside of a named entity * B-MIS, Beginning of a miscellaneous entity right after another miscellaneous entity * I-MIS, Miscellaneous entity * B-PER, Beginning of a person’s name right after another person’s name * I-PER, Person’s name * B-ORG, Beginning of an organisation right after another organisation * I-ORG, Organisation * B-LOC, Beginning of a location right after another location * I-LOC, Location\n\nner = pipeline('ner')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFirst using their example:\n\nsequence = (\"Hugging Face Inc. is a company based in New York City. Its headquarters are in DUMBO, therefore very\"\n    + \"close to the Manhattan Bridge which is visible from the window.\")\n\n\nfrom pprint import pprint\n\n\nfor entry in ner(sequence):\n    pprint(entry)\n\n{'entity': 'I-ORG', 'index': 1, 'score': 0.9995632767677307, 'word': 'Hu'}\n{'entity': 'I-ORG', 'index': 2, 'score': 0.9915938973426819, 'word': '##gging'}\n{'entity': 'I-ORG', 'index': 3, 'score': 0.9982671737670898, 'word': 'Face'}\n{'entity': 'I-ORG', 'index': 4, 'score': 0.9994403719902039, 'word': 'Inc'}\n{'entity': 'I-LOC', 'index': 11, 'score': 0.9994346499443054, 'word': 'New'}\n{'entity': 'I-LOC', 'index': 12, 'score': 0.9993270635604858, 'word': 'York'}\n{'entity': 'I-LOC', 'index': 13, 'score': 0.9993864893913269, 'word': 'City'}\n{'entity': 'I-LOC', 'index': 19, 'score': 0.9825621843338013, 'word': 'D'}\n{'entity': 'I-LOC', 'index': 20, 'score': 0.9369831085205078, 'word': '##UM'}\n{'entity': 'I-LOC', 'index': 21, 'score': 0.8987104296684265, 'word': '##BO'}\n{'entity': 'I-LOC',\n 'index': 29,\n 'score': 0.9758240580558777,\n 'word': 'Manhattan'}\n{'entity': 'I-LOC', 'index': 30, 'score': 0.9902493953704834, 'word': 'Bridge'}\n\n\nImpressive, especially how it recognizes DUMBO as a location. (side note, I actually visited that area in my first trip to NYC last year).\nLooking forward to trying out these transformers more in the future!"
  }
]